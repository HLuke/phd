\documentclass[authoryear,review,3p]{elsarticle}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[normalem]{ulem}

\usepackage{lineno,hyperref}
\usepackage{amsmath,amsfonts,amssymb}

\usepackage{listings}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage[acronym,nomain]{glossaries}

\begin{document}

\begin{frontmatter}

\title{Statistical Learning of Biological Structure in the Human Brain}

\author{Dr. med. Danilo Bzdok}

\end{frontmatter}

\bigskip
\bigskip
\bigskip
\centerline{
"But above all, master technique and produce original data; 
all the rest will follow."}
\centerline{Santiago Ram\'{o}n y Cajal}

\bigskip
\bigskip
\bigskip

\textbf{Supervisors\\}

Prof. Dr. rer.-nat. Stefan Conrad, Natural Science Faculty, HHU D\"usseldorf, Germany\\

Prof. Dr. med. Simon Eickhoff, Medical Faculty, HHU D\"usseldorf, Germany\\

Dr. Bertrand Thirion, INRIA, Saclay, France\\


\bigskip

\newpage
\section*{Publications related to the present dissertation}
\linebreak
\textit{Original papers}

\textbf{Bzdok D}, Grisel O, Eickenberg M, Thirion B, Varoquaux G.
Semi-supervised Factored Logistic Regression for High-Dimensional
Neuroimaging Data. Under review at NIPS.

\textbf{Bzdok D}, Grisel O, Eickenberg M, Varoquaux G, Poupon C, Thirion B.
Network-network architecture: Generative models of task activity patterns.
Under review at Cerebral Cortex.

Bludau S*, \textbf{Bzdok D*}, Gruber O,
Kohn N, Riedl V, Müller V, Hoffstaedter F, Eickhoff SB.
Medial prefrontal aberrations in major depressive disorder
revealed by cytoarchitonically informed voxel-based morphometry.
\textit{American Journal of Psychiatry}, in press. *equal contributions

\textbf{Bzdok D*}, Hartwigsen G*, Reid A, Eickhoff SB.
A hierarchy of the left inferior parietal lobe in social cognition and
language.
\textit{Neuroscience and Biobehavioral Reviews}, in press. *equal contributions

\textbf{Bzdok D}, Heeger A, Langner R, Laird A, Fox P, Palomero-Gallagher,
Vogt BA, Zilles K, Eickhoff SB.
Subspecialization in the human posterior medial cortex.
\textit{Neuroimage}, in press.

Eickhoff SB, Laird AR, Fox PT, \textbf{Bzdok D}*, Hensel L*.
Functional segregation of the human dorsomedial prefrontal cortex.
\textit{Cerebral Cortex}, in press. *equal contributions

\textbf{Bzdok D}, Langner R, Schilbach L, Laird AR, Fox PT, Zilles K, Eickhoff SB.
Characterization of the temporo-parietal junction by combining data-driven
parcellation, complementary connectivity analyses, and functional decoding.
\textit{Neuroimage}, 2013.

\bigskip
\textit{Review and opinion papers}

Eickhoff SB, Thirion B, Varoquaux G, \textbf{Bzdok D}.
Connectivity-based parcellation: critique \& implications.
\textit{Human Brain Mapping}, in press.

Eickhoff, SB \& \textbf{Bzdok D}.
Neuroimaging and modeling. Where is the road to clinical application?
\textit{Der Psychiater}, 2014, in press. 

Eickhoff SB \& \textbf{Bzdok D}.
[Statistical meta-analyses in imaging neuroscience.]
\textit{Klinische Neurophysiologie}, 2013, 44:199-203.

\bigskip
\textit{Book chapters}

\linebreak
\textbf{Bzdok D} \& Eickhoff SB.
Statistical learning of the neurobiological of schizophrenia.
In: \textit{The neurobiology of schizophrenia}, Springer, Heidelberg.

\newpage

\section*{1 Introduction}

\subsection*{1.1 Analytical versus heuristic views of nature}

The world around us is complex and volatile.
%
A large proportion of human research efforts are meditated by the 
"the unreasonable effectiveness of mathematics in the natural sciences."
This was phrased by the Hungarian-American
physicist, mathematician, and Nobel laureate Eugene P. Wigner (1960).
The language of mathematics is a powerful tool to
describe, formalize, and predict phenomena in nature.
The author emphasizes that it is not imperative that
natural regularities exist in the world. He goes on to
say that it might be even more surprising that humans can actually
find these regularities and use them to their advantage.
Similarly, Albert Einstein said:
“The most incomprehensible, is that the world is comprehensible.” 
%
Starting from human-conceived axiomes
we have derived always more complicated
properties of and relationships between mathematical objects by formal proofs
(Connes A., "A view of mathematics").
A logical pyramid of theoreoms is built that lead to always
more general asserations.
We also have detailed knowledge of the limitations of these mathematical
assertions.
%
On the one hand,
an identical regularity can often be equally well described in very distant
branches of mathematics.
On the other hand,
identical mathematical conclusions have also reemerged from derivation of
a priori unrelated assertions.
%
Indeed, the same formal language has proofed very apt in
the study of completely unrelated topics and diverging scientific disciplines.
This includes movements of celestial objects in the universe studied in astronomy,
metabolism pathways governing the inner life of the cell studied in biochemistry,
and
ABC.
%
Many rules about the world can thus be perfectly grasped
(Hardy, Apology).
As another simple example,
Fibonacci numbers (1, 1, 2, 3, 5, 8, 13, etc.)
reappear in many natural phenomena.
The number of petals of a flower and the spirals of a pinapple tend
to be Fibonacci sequences.
The family tree of honey bees is also governed by Fibonacci regularities.
Even the proportions of human finger bones follow this formalism.
Knowledge of such mathematical regularities
allows to impose logical structure on the external world.
%
It remains an unresolved philosophical debate whether we have
\textit{discovered} or \textit{invented} mathematics.
Yet, there is probably no doubt
that mathematical conceptualization
evolves as a feature of human cultural evolution (Tomasello, 2001).
Even the most abstract mathematical concepts can
be exchanged between individuals. Consequently,
they can be easily passed on across generations and geographical distances.
It is interesting to note that
there is usually consensus among mathematicians
about the architecture of their discipline.
From an anthropological perspective,
mathematical formalism
appears to be one of the most
powerful tools and most defining properties of the human species
(S. Dehaene, "The Number Sense").
Indeed,
Eugene Wigner concludes his praise of equations with the following words:
"The miracle of the appropriateness of the language of mathematics
for the formulation of the laws of physics is a wonderful gift
which we neither understand nor deserve. We should be grateful for
it and hope that it will remain valid in future research [...]."
(1960, p. 14).


However, this dogma has repeatedly been challenged formally
and empirically.
%
In formal approaches,
mathematics were shown incomplete and inherently contradictory because,
in any axiomatic systems, some true assertions cannot be proven
("incompleteness theorems", G\"odel, 1931).
Additionally,
it is possible to define a real number with equidistributed
digits that can however not be computed
("Chaitin's constant $\Omega$", Chaitin, 2006).
%
In empirical approaches,
the omnipotence of mathematical equations has been challenged
as the
best possible way to describe and predict nature
(Halevy et al., 2009; Hinton/LeCun/Bengio, 2015; Pietsch, 201?).
%
This shift in the scientific discourse is made explicit by
three recent empirical observations:
\begin{enumerate}
  \item Sophisticated, more accurate models can be outperformed by
  simple models that are fit with massive training data.
  \item Simple models trained with rich data can outperform
  models that have been designed according to
  extensive domain expertise.
  \item There appears to be a minimal threshold of training data
  for deriving models that suddenly exhibit emergence properties.
\end{enumerate}

One prominent example for such heuristic modelling of nature is
automatic translation of human languages.
Translation systems based on 
human-derived systems of grammar rules have perhaps never achieved
satisfactory success (P. Norvig, 2011, "On Chomsky").
That is, analytical approaches based on
thousands of books pages storing domain expertise in form of
deterministic rules appeared insufficient for building
language models that can cope with real-world settings.
%
Statistical machine translation was more successful
by implementing probabilistic hidden Markov models
as a heuristic approach.
The class of hidden Markov models (HMM)
thus became a dominanting feature
of computational linguistics.
%
Today, virtually all professional translation software solutions
for both written and spoken language
are enabled by heuristic mathematical models.

As an example from human biology,
we currently have only few means to predict the
toxicity of environmental chemicals and
potential effects of new drug compounds on health.
%
The complex and unknown phenonon in nature here pertains
to the causal link from
a protein's \textit{known} primary structure
(i.e., 1D string of aminoacids)
to
its \textit{unknown} tertiary/quaternary structure
(i.e., the 3D folding of the aminoacid string that subserves function).
%
Of the millions of existing proteins
we only know the tertiary/quaternary structure of
approximately 30.000 ones.
The structural configuration is however necessary to identify the position of
bindings sites for protein-protein interaction.
Knowledge of these sites in 3D space is crucially important
to infer that protein's interplay with the human body.
%
In this case,
the learning problem is to
derive a computational model from
massive pairs of known primary protein structure and
known protein properties, including toxicity,
by intentionally treating the 3D structure
as a black box.
%
State-of-the-art neural network algorithms
have very recently solved this heuristic learning problem
better than any previous approach in acadmia or industry
(Dahl et al., 2014; Unterthiner et al., 2015).
These investigators thus showed that biological activity
can be reliably predicted from single aminoacid strings
even without recourse to biological domain expertise.
%
There might currently not exist
an analytical counterpart to such
structure-function mapping of proteins.
%
%
Similar probabilistic scenarios would include predicting
political outcomes,
optimizing advertisement strategies,
algorithmic trading in stock markets, and
controlling self-driving cars.


In sum,
humans can create machines to derive
algorithmic predictions from the data.
Observing a phenomenon in nature a sufficient number of times
might be sufficient to
algorithmically extract
the heuristics of its interaction behavior with the world.
This has recently entailed a shift
of attention from model complexity to data complexity.
In the absence of any analytical alternative access,
simple mathematical models can thus automatically
formalize diverse classes of natural phenomena
depending on the quality and quantity of
the available data resources.



\subsection*{1.2 Two cultures of statistical modelling}

Statistics is a branch of mathematics that has arguably been the
overall most successful information science.
Statistics aim at extracting information from data
about the mechanisms in nature that generated these data.
%
Given its eclectic character, it may come as no surprise that statistics
has developed both analytical and heuristic strategies
to model regularities of phenomena in nature.
Yet, analytical and heuristic statistical cultures
have emerged independently (Breiman, 2011).
They differ with regard to historical origin, mathematical foundation,
and modelling goal.

The overwhelming majority of statisticians
follow an analytical regime by
adhering to \textit{classical statistics} (CS) for
\textit{data modelling}.
They hold that the phenomenon under study can be viewed as a black box
whose inner workings can be described by a small set of
underlying variables.
It is up to the statistician in charge
to choose the model that best reflects nature.
Data are then used to estimate the parameters of that pre-specified model.
Classical statistics have dominated research at the universities
for almost 90 years now.
%
Well known members of the CS family include for instance
Student's t-test, ANOVA,
and Chi-squared test.
\textit{Statistical hypothesis testing} has been introduced in the beginning
of the last century (Fisher, 1925; Neyman and Pearson, 1928).
The same approach is still practiced today in its original form (Goodman, 1999).  
%
The ensuing \textit{p-value} measures how likely it is
to observe the data at hand
assuming the non-preferred null hypothesis ($H_0$)
to find indirect evidence
for the preferred alternative hypothesis ($H_1$).
%
Despite the prevailing presence of p-values,
it has not been conceived by Fisher as an acid test
to judge existing versus non-existing effects in nature.
Rather, the intention was a preliminary tool to
filter which potential effects should be more explicitly tested (Nuzzo, 2014).
%
Notably, if the hand-selected model is a bad description of
the natural phenomenon under study,
the drawn conclusions may be wrong.
%
Nevertheless, statistical hypothesis testing probably fit perfectly
in its time of conception and adoption.
It was designed for use with mechanical calculators
(Efron and Tibshirani, 1991). Gaussian distributional assumptions
have been very useful in many instances for
mathematical convenience and, hence, computational tractability.
Additionally, it suited perfectly the Popperian view of
critical empiricism in academic discourse:
scientific progress is to be made by continuous replacement of current
hypotheses by always more explanatory hypotheses
using verification and falsification (193X).
The rationale behind hypothesis falsification
is that even a lot of evidence cannot confirm
a given theory in an \textit{inductive} way, 
while a single counter example is able to proof a theory wrong
in a \textit{deductive} way.
%
In sum,
classical statistics was mostly fashioned
for problems with few data points that can be modeled 
by plausible models with a small number of parameters chosen by the
investigator.


In contrast, only a small minority of statisticians
follow a heuristic regime by
adhering to \textit{statistical learning} (SL) for
\textit{algorithmic modelling}.
This statistical framework is frequently adopted by computer scientists,
physicists, ingenieurs, and others without formal statistical brackground
that are typically working in industry
rather than academia (cf. Daniel and Wood, 1971).
%
They hold that natural phenomena
can be studied by estimating regularities in the inputs and
outputs to the black box without making assumptions
about its interal "true" mechanisms.
A statistical model is thus derived that expresses
relationships between the input and output variables
whose parameters are learned by training data.
Put differently, a new function with potentially thousands of
parameters is created
that can predict the output from the input alone.
The input data thus need to represent
all relevant configurations of the examined phenomenon in nature.
%
Please note that SL here summarizes the seemingly more specific terms
data-mining, pattern recognition, artificial intelligence,
and machine learning that are often employed inconsistently.
Members of
the SL family include for instance k-means clustering,
Lasso/Ridge regression, and support vector machine classification.
The independent historical origin of CS and SL families is even
witnessed by the most basic terminology. 
In the CS literature inputs to statistical modeling
are classically called \textit{independent variables}
or, more recently, \textit{predictors},
while these are commonly referred to as \textit{features}
in the SL literature (Hastie et al., 2011).
%
When evaluating whether a certain problem is a possible target for SL
three requirements come into play (Abu-Mostafa, 2012):
\begin{enumerate}
  \item A regularity exists.
(if there is no pattern, then there is no harm in trying SL).
  \item The regularity cannot be formalized analytically
(otherwise one can still apply SL, but it might not create the best model).
  \item We have data on the problem (the more, the better).
\end{enumerate}
%
This statistical framework led to a surge of new
computer-intensive statistical techniques since 1980
that are less concerned with mathematical tractability (Efron, 1991).
In the 95'ies suppor vector machines (SVMs)
proofed to be better than the back then de-facto-standard neural networks
in both classification and regression problems (Valpnik, 19??).
This development has been flanked by changing properties of datasets that
are always higher-dimensional (i.e., more features per observation)
and
based
on larger samples (i.e., more observations).
This is a trend that is not specific to
neuroimaging research but also takes places
in other scientific disciplines,
including weather forecasting and economic predictions
(Manyika et al., 2011).
In sum,
statistical learning was mostly fashioned
for problems with many data points with largely unknown
data generating processes
that are emulated by a mathematical function
created on-the-fly by a machine.


Importantly, certain statistical
methods cannot be easily categorized by the CS-SL distinction.
Statistical methods span a continuum between the two poles of CS and SL
(Jordan/Frontiers in Massive Data, p. 61).
Nevertheless, the two families of statistical methods
can be easily distinguished by a number of archetypical properties.
Bayesian statistics do for instance are orthogonal to the CS-SL distinction
and can be adopted in both methodological families in various fashions.
Neither can the terms univariate versus multivariate
(i.e., relying on one versus more than one input variable)
be clearly grouped into either CS or SL.
%
Neither CS nor SL can generally be considered superior.
This is captured by the \textit{no free lunch theorem}
stating that no single statistical strategy can#
consistently do better in all circumstances (Domingos, 2012; Wolpert, 1996).
The challenge relies in choosing
the statistical approach that is best suited
to the neurobiological phenomenon under study and the neuroscientific research object at hand.


Regarding their modelling goals, CS and SL exhibit various differences.
CS typically aims at modeling the black box by making a set of
accurate assumptions about its content,
e.g. the nature of the signal distribution.
Contrarily, SL typically aims at finding any way to model
the output of the black box from its input
while making the least assumptions possible (Abu-Mostafa et al., 2012).
In CS the phenomenon is therefore treated as partly known
(i.e., the stochastic processes that generated the data),
whereas in SL the phenomenon is treated as complex,
completely unknown, and partly unknowable.
It is in this way that CS tends to be
analytical
(i.e., imposing mathematical rigor on the phenomenon),
whereas SL tends to be
heuristic
(i.e., finding useful approximations to the phenomenon).
CS assumes a given statistical model at the beginning of the investigation,
whereas in SL the model is
generated in the process of the statistical investigation.
In more formal terms,
CS therefore closely relates to parametric statistics
for confirmatory data analysis
whereas SL closely relates to non-parametric statistics
for exploratory data analysis.
In more practical terms, CS is typically applied to experimental data,
where the variables were controlled by the investigator
(i.e., system under studied is perturbed),
while SL is typically applied to observational
data without such structured influence
(i.e., system is left unperturbed) (Domingos, 2012).
The work unit for CS is the quantified
significance associated with a statistical
relationship between few variables given a pre-specified model.
The work unit for SL is the quantified robustness of patterns
between many variables or, more generally,
the robustness of \textit{special structure} in the data (Hastie et al., 2011).
CS therefore tests for a particular structure in the data,
whereas SL explores and discovers structure in the data.
Formally, CS implements data modeling by
imposing an a priori model in a top-down fashion,
whereas SL implements algorithmic modeling by fitting
a model as a function of the data at hand in a bottom-up fashion.
%
Intuitively, the truth is believed
to be in the model (cf. Wigner, 1960) in a CS-contrained world,
while it is believed to be in the data
(cf. Halevy et al., 2009) in a SL-constrained world.
As a drastically oversimplified, yet useful, conclusion,
CS preassumes and tests \textit{a model for the data},
whereas SL learns \textit{a model from the data}.
%
Indeed,
both human and computer learning are theoretically
more conceivable in a probabilistic rather than
deterministic sense
(Abu-Mostafa et al., 2012; Dayan et al., 1995; Friston, 2010; Gregory, 1980).
Moreover, each probabilistc model can be viewed as a superset
of a deterministic model (P. Norvig, "On Chomsky").
%
Taken together,
CS assumes that the data behave according to known mechanisms,
whereas SL exploits
computer algorithms to avoid the a-priori
specifications of data mechanism.



\subsection*{1.3 The human brain as a complex phenomenon in nature}
The human brain is a prime example of
a black box that is complex, mysterious, and probably in parts unknowable.
It is frequently proposed that
the human brain might be the most complex object in the known universe
(Nature editorial, october 2014).
With the language from above,
the human brain might constitute a phenomon in nature that
can \textit{not} be perfectly grasped by mathematical formalism alone.
More concretely,
the \textit{most pertinent structure} we should assume for the human brain,
when measured by
contemporary functional neuroimaging techniques (cf. next passages),
is currently unknown.
Hence, the neuroimaging access to brain science can readily be framed as
a problem of \textit{representation learning} (Bengio, 2014).
It is conceivable
that this task can be solved without exhaustive
multi-level neurobiological knowledge (Bostrom, 2014),
which is always more supported by empirical evidence
(e.g., Helmstaedter, M. et al. "Connectomic reconstruction" 2013 Nature).
This contention is embraced by the present disseration.


From a global perspective,
the genetic difference between our genetic equipment and that
of our closest ancestors, the non-human primate, turns out not to be 
particularly striking.
This has encouraged the conviction that one or very few key genetic
adaptations in the primate lineage have unchained an avalanche
of cognitive and cultural inventions that led to today's civilization
(Tomasello, 2001).
That is, the human species might be much more defined by the
increasingly fast cultural evolution rather the ramifications
of slow biological evolution.
Key cognitive improvements, such as the emergence of verbal language,
might have fueled cultural improvements that, in turn, enabled
further cognitive improvements et cetera pp.
This form of \textit{online learning} is a very plausible and concisive
property of intact tissue of the central nervous system.
%
As a first challenge in brain science,
it might therefore be impossible to cleanly dissect the
the nature-nurture interplay into independent contributing factors that act
during
phylogeny (i.e., development of the species)
and
ontogeny (i.e., development of an individual organism).
%
In this sense,
investigating the limits between "nature" and "culture" in humans
might equate with asking a paradoxical question
(Dehaene \& Cohen, 2007; Watzlawick, Logics/Communication).
%
Instead,
a necessary factor for the high level of abstraction in human culture
might have precisely been the inextricability, due to bidirectional influence, of
neurobiological plasticity and relentless cultural exchange
between human individuals
in a non-stop, autopoietic optimization process
(Vygotsky 1978, "Mind in Society"; Luhmann 1984, "Soziale Systeme";
Bengio 2013, "Evolving Culture").


Given this acceleration in recent cultural evolution
(cf. Paul Virilio, "Open Skype"),
it might be
rather unlikely that the human brain has developed dedicated
neuronal populations to subserve the panoply of novel behaviors.
Rather, evolutionarily recent mental tasks
(e.g., reading and writing, explicit pedagogy, and mathematics)
are realized by recombining low-level circuits that initially
developed for other functional roles.
This view has become known as
"neural reuse" and "neural recycling"
(Anderson, 2010; Dehaene & Cohen, 2007).
Non-human primates are lacking many of the sophisticated
mental operations that
are crucially important for maintaining human societies
(Mesulam, 1998; Tomasello, 2003).
Indeed,
the "social brain hypothesis" states that our
computationally powerful brains are not an adaptation to
solving problems posed by the physical environment,
but for successfully coping with increasingly complex social systems
(Byrne et al. 1988; Dunbar and Shultz 2007b).
Yet, it is becoming increasingly clear that socialaffective processing
in the human brain is probably realized by domain-general
brain regions and networks not specific to maintaining social interactions
(Bzdok et al., Neurobiology/Morality; Behrens et al., 2009 Computation).
These considerations entail a second challenge in brain science:
It is probably impossible to know what purpose neural
processing in a given part of the brain has originally evolved to serve.
We can only observe external manifestations and correlates of
this latent biological purpose.


Importantly,
no two human brains are alike.
Quite the opposite,
they differ with regard to
the morphology of gyri and sulci,
the topology of cytoarchitectonically and chemoarchitectonically
distinguishable brain areas,
the axonal connections linking these brain areas,
as well as the history of their sensory inputs.


The extent of a brain area and its inter-individual variability
can be quantitatively examined with its relation to cognition and behavior,
that is,
performence in psychological tasks in the healthy or diseased brain.
For instance, the volume of the amygdala is linked to
interindividual differences in memory performance as well as
many other (temporally transient) states and
(temporally enduring) traites.

As another challenge in brain science,
it is currently unknown how
brain-behavior correlations are mediated.

The renowed neuroanatomist Santiago Ram\'{o}n y Cajal wrote (1909):
"The complexity of the nervous system is so great,
its various association systems and cell masses so numerous and
complex, and challenging,
that understanding will forever lie beyond our most committed efforts."

More concretely, it remains largely elusive whether distinct behavioral differences
are associated with changes of
cell bodies, dendrites, axonal connections, and/or glial cells
(Kanai et al., 2011).
%
From a fundamentally neurobiological perspective,
we do not have clear understanding of how
this set of microstructures interact to
solving neural computation problems.
From a methodological perspective,
the conventional volumetric modelling techniques are naïve to
many types of possible morphological differences.
For instance,
it is currently difficult to statistically grasp
inversely proportional left and right hemisphere volumes
or
a condition that randomly affects either the left of the right brain
per individual
(Ashburner et al., 2011).



Worth to be proposed as an independent challenge of
brain science, the secret of interhemispheric
asymmetry is yet to be unveiled.
The connectivity differences between the left and right brain are
for instance currently underresearched.
They are even hardly known in the monkey (Stephan, 2007)
that usually serves a fallback system for human
connectivity investigations (Mesulam, 2012 "The evolving").
In humans, the majority of homologous areas feature
direct anatomical connections.
Nevertheless, as two textbook examples,
why the language and attention processes typically lateralize to
the left and right, respectively,
is understood only in fragments
(Corbetta 2000; Stephan et al., 2003; Price et al., 2010).







Experimental

Until
19 about 20 years ago, the most common approaches
20 to localize human brain functions consisted in
21 lesion studies and direct brain stimulation during
22 neurosurgical interventions. 

G. M. Shepherd: "Nothing in neuroscience makes sense except in the light of behavior"
->
Edward O. Wilson: "Disturb Nature and see if she reveals a secret."
makes it unlikely that we will reach exhaustive understanding of
the human brain by
\textit{observational}, as opposed to \textit{interventional}, methodology
alone.

Regarding the functional dimension, Franz and Lashley (1917) were probably the first to induce focal lesions in circumscribed cortical areas in rats and measure the ensuing effects on the performance of behavioral tasks. Accidental lesion in the human brain likewise provided hints regarding the cortical localization of psychological processes (Harlow 1869). 
it was the use of psychological tasks that was crucial in these studies.

It is a limitation of these studies that they attempt to derive the normal function of an area from the effects of damage to that area.

On a macroscale, the metabolic baseline turnover is not equally distributed across the brain. The metabolic demands are variable across brain regions and (to a smaller extent) across time. The highest metabolic consumption locates, first, to the posterior cingulate cortex extending into the adjacent retrosplenial cortex and precuneus and, second, to the medial prefrontal cortex extending into the anterior cingulate cortex (Raichle et al., 2001; Reivich et al., 1979).
-> but we have no idea why

young2000:
Yet the link between connectivity and lesion e¡ects remains almost completely opaque.

In the context of inferences from lesion e¡ects, it is a strong temptation to believe that an experimentally induced lesion causing a decrement in a behaviour does so directly through the impairment of the information processing functions of the lesioned structure. However, indirect network-mediated e¡ects, if present, suggest that it is unsafe to assume that a lesion has its detrimental e¡ect on behaviour by virtue of the e¡ect of the lesion on processing local to the lesion site

Hence, if the simple propagation e¡ects of lesions derived from the simulation in 2 obtain in the real brain network, neither single nor double dissociation derive reliable information about the functions mediated by brain structures.

Accidental lesion in the human brain likewise provided hints regarding the cortical localization of psychological processes (Harlow 1869). 

a large fraction of lesion patints are stroke patients
-> spectrumm of lesion patterns seriously limited because of
anatomy of vascular system

Given the lack of \textit{focal} structural changes in almost all psychiatric diseases

It appears that many structures are activated by different tasks across different task cat- egories and cognitive domains. 

Mesulam:
In the fish, frog and pigeon, decortication produces little change in sensation or locomotion (Ferrier, 1876; Northcutt, 1978). Extensive neocortical lesions in hamsters, cats and monkeys cause considerably lesser and more reversible functional deficits than analogous lesions in humans (Lashley, 1952; MacLean, 1982).

These observations point to the existence of a trend towards a progressive corticalization of function

structure (by probabilistic histologi- cal mapping), connectivity (by coactivation mapping), and function (by meta-data profiling). The present work might thus be the first to provide direct evidence that structural, connectional, and functional characteristics reflect three different viewpoints on the same heteroge- neity of a particular brain region [Passingham et al., 2002].

The literature of the 1940s and 1950s expressed considerable ambivalence about the status of the cerebral cortex in mental functioning and tended to ignore the seminal contributions of Hughlings Jackson, Ferrier, Charcot, Dejerine, Wernicke and Liepmann. Wilder Penfield (1938), for example, had attributed the highest levels of integration to thalamic activity and Karl Lashley (1952) had raised serious doubts about the existence of functional specializations in the association cortex, even in the monkey.

The field of neuroscience had been primed to anticipate such a sequential organization through the work of Hubel and Wiesel (1965), who had demonstrated a hierarchy of simple, complex and hypercomplex neurons in the primary visual cortex, each successive level encoding a more composite aspect of visual information.
-> nothing similar in higher cortices



Everyday experiences unfold in multiple modalities. The establishment of a durable record of experience, and its associative incorporation into the existing base of knowledge, necessitate multimodal integration. The desirability of such integration (or binding) had been articulated and its presence postulated on multiple occasions
-> binding problem (Engel, 2001; Varela, 2001):
how is the pstially idstributed but temporally coherent electrical activity
from a large number of elementary neural componments integrated to functional activity

Transmodal areas are not necessarily centres where convergent knowledge resides, but critical gateways (or hubs, sluices, nexuses) for accessing the relevant distributed information (Mesulam, 1994c). Paradoxically, they also provide ‘neural bottlenecks’ in the sense that they constitute regions of maximum vulnerability for lesion-induced deficits in the pertinent cognitive domain.







DMN:

example of serendipity / A property of the brain that we might not have
discovered without the advent of brain imaging methods.

The default-mode network - we are not even close to knowing
what this baseline/house-keeping activity network is doing

A resting-state connectivity derived network that was initially proposed to be consistently decreasing neural activity during experimental paradigms requiring stimulus-guided behavior. It is now believed to be mostly, but not exclusively, active in the absence of external stimulation. 
15 years ago neuroscientists discovered a unique brain network. The so-called default-mode network (DMN) appeared to be exclusive in decreasing neural activity consistently during experimental tasks.
It was later even argued that this network is systematically anti-correlated with brain regions active during task performance. This was recently challenged by repeated reports of brain regions exhibiting both task-constrained and task-unconstrained increases in neural activity. On the one hand, the DMN is now believed to be mostly, but not exclusively, active during absent external stimulation. On the other hand, the DMN is today known to also increase neural activity consistently during a small set of complex cognitive tasks. This includes the contemplation of others’ and one’s own mind states, as well as scene construction processes when envisioning past, fictitious, and future events. It was speculated that the human brain might have evolved to, by default, predict environmental events using mental imagery. Constructing detached probabilistic scenes could thus influence perception and behavior by estimating saliency and action outcomes. This would invigorate a possible relationship between the physiological baseline of the human brain and an introspective psychological baseline.
The DMN defies our neuroscientific intuitions and challenges established methodology. This brain network likely represents a physiological instantiation of a human beings' default mental repertoire, the nature of which remains largely obscure.

This functional segregation is underlined by the here observed anti-correlation between aRTPJ and pRTPJ. Please note that the repeatedly shown anti-correlation between the (aRTPJ-related) saliency and (pRTPJ-related) default-mode network (Fox et al., 2005a; Kelly et al., 2008) was here derived from, and is thus directly related to, different functional modules within RTPJ. Indeed, goal-directed task performance improves with increased activity in saliency-related areas and decreased activity in default-mode areas (Weissman et al., 2006). Conversely, increased activity in areas of the here delineated pRTPJ network were linked to increased occurrence of task-independent thoughts (i.e., mind-wandering) during task execution (Mason et al., 2007). Two fMRI studies employing Granger causality analysis further corroborated the anti-correlation by indicating negative influence of the default-mode on the saliency network (Pisapia et al., 2012) and vice versa (Sridharan et al., 2008). Taken together, present and previous findings converge on the tentative notion that the human RTPJ contains two functionally complementary modules that belong to antagonistic networks potentially underlying externally versus internally oriented processing.

The probably best-known RSN is the default-mode network (DMN) (Gusnard et al., 2001) that was discovered with and mainly characterized by RS connectivity. The DMN was initially proposed to be exclusive in decreasing neural activity consistently during experimental paradigms requiring stimulus-guided behavior. This RSN was therefore proposed to reflect the neural correlates underlying unfocused every-day mind wandering (Schacter et al., 2007). This, in turn, would suggest a close correspondence between the physiological and psychological baseline of the human brain (Schilbach et al., 2008). It was later even argued that this network is systematically anti-correlated with brain regions more active during task performance (Fox, et al., 2005). This was recently challenged by repeated reports of brain regions exhibiting both task-constrained and task-unconstrained increases in neural activity (Buckner, et al., 2008). More specifically, the DMN is now known to consistently increase neural activity during a small set of complex cognitive tasks, including social cognition, autobiographical recall, and spatial navigation (Bzdok, et al., 2013; Spreng, et al., 2009). Task-, RS- and meta-analysis-informed research on the DMN thus corroborated that this particular network is consistently decreases activity during externally focused mental tasks and typically increases activity during a small set of internally focused mental tasks.

Mental imagery provides one of at least three settings where the activation of sensory areas can transcend the constraints imposed by external reality. The two other examples include working memory, where the representation of a sensory experience is prolonged beyond the duration of the stimulus, and anticipatory paired-associate responses, where sensory representations become activated in expectation of an event which is not yet present in reality. (MES)







As Roger Shepard and Steven Kosslyn have shown, the time needed to rotate or explore these mental images follows a lin- ear function of the angle or distance traveled. Mental trajec- tory imitates that of a physical object.












-> Oscillations:

Neural circuits often undergo oscillatory activity patterns

a intrinsically probabilistic process
-> perception depends on the phase of intrinsic oscillations cycles

Much based on bottom-up and top-down mediated somewhat serial
procesign in the brain. Oscillation structure however suggest simultaenously
active equilibriums


In animals, firing rates of neuronal populations may not be
directly related to perception or awareness

Feed- forward and feedback networks predict well what happens next. Oscillators are very good at predicting when.

reflect both input sensitivity and output formation
alrgely inter-neuron mediated
neural synchrony as a communication code (Singer, 1999).

several rhythms can temporally coexist in the same or different structures and interact with each other

high frequencies -> small neuronal space; slow oscillations -> large networks are recruited

They occur in specific frequency bands, which are functionally specialised, and appear to be modulated in a regionally- specific way.

Cognitive functions may also be dependent on shifts in the proper- ties of oscillations.

Whereas the spiking of single cortical principal neurons typically displays Poisson statistics (35), their assembly behavior is often characterized by oscillatory properties

The “default” state of the unperturbed, sleeping brain is a complex system of numerous self-governed oscillations, par- ticularly in the thalamocortical system

*Spontaneous oscillatory activity* can occur in the absence of any stimulus or overt behavior, but can be modulated by various conditions. One prominent example of such spontaneous activity refers to oscillations in the alpha band (7–13 Hz), commonly observed at sensors over parietal and occipital brain areas while subjects are at rest (Berger, 1929; De Munck et al., 2007). These spontaneous oscillations are further modulated by context, for instance, when subject’s have their eyes closed alpha-power usually increases (Ciulla, Takeda, & Endo, 1999).

Interneuron networks generate rhythmic synchronization regardless of whether their excitatory drive is rhythmic

- Siegel2012: A particular frequency band may reflect one canonical computation in some anatomical networks, but may reflect a different mechanism in other networks. There may even be characteristic fre- quencies for specific anatomical networks that do not generalize to other networks. 

in mammalien cortex: power density of local field potential is inversely proportional to the frequency** -> This 1/f power relationship implies that pertur- bations occurring at slow frequencies can cause a cascade of energy dissipation at higher fre- quencies and that widespread slow oscillations modulate faster local events

perturbed by environmental stimulation
task-realted cortical activity normally induced by,but not phase-locked
to, external events

oscillations and their behavioral corelates are largely preserved
throughout mammalian evolution.
Feeforward and feedback networks predict well what happens next -
oscillators predict well when.

Different frequency badns -> implicated in same cognitive process.
Different cognitive proceses invovled in the same frequency ranges.
one frequency -> could correspond to one specific computation in
one anatomical networks, but a different one in another netuwork

Interactions between frequ: phase-p, p-a-, a-a- itneractions






- Argumente von DFG Antrag








In non-parametric modeling, predictions are calculated on the basis of all data. There is no detour over a parametric model that summarizes the data in terms of a few parameters.


However, there is still uncertainty about how the clinical symptoms of schizophrenia relate to the neurobiology of the human brain. 
there might be a non-trivial relationship in the psychiatric disorders between the neurobiological endophenotype and the clinical exophenotype as captured by the BDI, PANSS, and ADOS questionnaires.






PHILO


The intricacies any neuroscientists faces when articulating observation of phenomena in the brain appear closely related to Ludwig Wittgenstein's second main book Philosophical Investigations (1953/2001). The late Wittgenstein argues that confusion introduced by human language itself is the origin of most philosophical problems. The grammatical and lexical constraints of human language would be too tight to allow for unequivocal description of the diverse circumstances humans encounter. According to Wittgenstein the meaning of language is primarily defined by its practical use in concrete situations, rather than decontextualized abstractions necessarily pre-shared by the interlocutors. Meaning would not even be primarily engendered by reference to objects or their properties. Rather, meaning emerges as a social event between interacting interlocutors. The important implication here is that words and phrases might not have an objective meaning equally accessible to and understood by everybody. This is all the more the case for language descriptions of phenomena that do not occur in every-day reality. Discussing subtleties of abstract concepts, which can hardly be practically experienced, are frequent subject to ambiguity, thus leading to unnoticed misunderstanding and unresolvable paradoxes (cf. Bostrom, 2002; Gödel, 1931; Watzlawick et al., 1967). Biological processes in the brain are an instance of such not directly experienceable phenomena underdetermined by human language that entail interpretative conundrum.

language: relates to Wittgenstein‘s second main book (Philosphical investigations)
-> conceptual confusion introduced by language is at the root of many philosophical problems (actually contradicts most of first main book)
He alleges that the problems are traceable to a set of related assumptions about the nature of language, which themselves presuppose a particular conception of the essence of language. This conception is considered and ultimately rejected for being too general; that is, as an essentialist account of the nature of language it is simply too narrow to be able to account for the variety of things we do with language
-> He then sets out throughout the rest of the book to demonstrate the limitations of this conception, including, he argues, many traditional philosophical puzzles and confusions that arise as a result of this limited picture.
->The essential point of this exercise is often missed. Wittgenstein's point is not that it is impossible to define "game", but that we don't have a definition, and we don't need one, because even without the definition, we use the word successfully.
-> Wittgenstein's investigations of language lead to several issues concerning the mind. His key target of criticism is any form of extreme mentalism which posits mental states that are entirely unconnected to the subject's environment. For Wittgenstein, thought is inevitably tied to language, which is inherently social; therefore, there is no 'inner' space in which thoughts can occur. Part of Wittgenstein's credo is captured in the following proclamation: "An 'inner process' stands in need of outward criteria."
-> This follows primarily from his conclusions about private languages: similarly, a private mental state (a sensation of pain, for example) cannot be adequately discussed without public criteria for identifying it.
-> Wittgenstein repudiates many of his own earlier views, expressed in the Tractatus Logico-Philosophicus

Note that we considered all eligible BrainMap experiments because any pre-selection of taxonomic categories would have constituted a fairly strong a priori hypothesis about how brain networks are organized. However, it remains elusive how well psychological constructs, such as emotion and cognition, map on regional brain responses (Laird et al., 2009a; Poldrack, 2006).

There are more reasons why the words and concepts social neuroscientists use should receive increased attention. Conducting studies based on hypotheses that preassume the biological validity of traditional psychological categories, when, as a first example, comparing brain responses to "emotional" versus "cognitive" stimuli, constitute a strong a priori assumption about how brain networks are organized. Yet, it remains elusive how and to what extent psychological terms, such as emotion and cognition (Pessoa, 2008; Van Overwalle, 2011), map onto regional brain responses (Laird et al., 2009; Mesulam, 1998; Poldrack, 2006). As a second example, it is discussed whether social interaction is realized in the brain by simulation that occurs in response to perceiving others' external behavior (especially body movement), subserved by the so-called mirror neuron system, or by means of an allegedly more abstract conceptualization of others' internal states (i.e., thoughts), which could be subserved by theory-of-mind capacities. This debate makes the implicit, yet significant assumption that those two concepts have a discrete representation in neurobiology. Regarding the simulationist side (i.e. MNS), it is still a matter of debate whether humans have an analogue to the mirror neuron system known to exist in non-human primates (cf. Keysers and Gazzola, 2010). Regarding the theorist side (i.e. MENT), the neural network consistently recruited during theory-of-mind cognition is also consistently involved in an array of diverse psychological processes, including moral thinking, autobiographical memory retrieval and navigation (Bzdok et al., 2012b; Spreng et al., 2009). It should hence be carefully called into question whether terms such as "mirror neuron system" or "theory of mind" are an adequate choice of words to refer to discrete neurobiological processes in humans. Taken together, social neuroscience has so far heavily relied on terms and concepts that have been borrowed more traditional, non-neurobiological scientific disciplines. It is however largely unknown how well they grasp neurobiological processes underlying social interaction (cf. Bzdok et al., 2014; Engemann et al., 2012).

Conducting studies based on hypotheses that preassume the biological validity of traditional psychological categories, when, as a first example, comparing brain responses to "emotional" versus "cognitive" stimuli, constitute a strong a priori assumption about how brain networks are organized. Yet, it remains elusive how and to what extent psychological terms, such as emotion and cognition (Pessoa, 2008; Van Overwalle, 2011), map onto regional brain responses (Laird et al., 2009; Mesulam, 1998; Poldrack, 2006). 
-> CS is one huge implicit assumption:
the underlying semantics concepts of the forumulated hypotheses are true
This debate makes the implicit, yet significant assumption that those two concepts have a discrete representation in neurobiology. 

(Bzdok et al., 2012b; Spreng et al., 2009). It should hence be carefully called into question whether terms such as "mirror neuron system" or "theory of mind" are an adequate choice of words to refer to discrete neurobiological processes in humans. Taken together, social neuroscience has so far heavily relied on terms and concepts that have been borrowed more traditional, non-neurobiological scientific disciplines. It is however largely unknown how well they grasp neurobiological processes underlying social interaction

-> CS is one huge implicit assumption:
the underlying semantics concepts of the forumulated hypotheses are true






Epistemology (Carruthers, 2009; Dehaene, MBE 2007):

Biology has a small legacy of abstract theorizing.

three difficult questions:

a) Does the human brain possess enough resources to describe itself?

b) Not clear whether the human mind can reflect upon itself
by directly contemplating itself (i.e., introspection) or
indirectly contemplating an internalized self-model acquired through
interaction with others

c) And isn’t this enterprise of self-description of the laws of the mind by the mind itself intrinsically limited, or even contradictory or tautological? / self-reflexive


necessary but not sufficient


There are many intricacies about neurobiology and 
the mosaic knowledge that we currently have about it.
%
Despite ~200 years of brain research,
we are probably not even close to something like a unified theory of
brain function
that neuroscientists from different fields would accept
(Friston, 2010 "Free energy principle"; Bar, 2009 "Predictions").
This complicates the formulation of precise, neurobiologically
valid hypotheses that can be experimentally tested in targeted studies.
%
Therefore, it might be helpful to use heuristics-establishing
statistical approaches for pattern discovery instead of
classical statistics alone.
%
Discovering the mystery of the brain \textit{exclusively} by
successive falsification of
entirely human-conceived,
intimately language-dependent,
and dichotomic hypotheses
might be a viewed as hybris by some (cf. Cajal, 1909).
%
The present dissertation is built on the assumption that
we might not reach an \textit{exhaustive analytical understanding}
of the brain any time soon
and that a more pragmatic access relies in the \textit{heuristic
approximation of brain mechanisms} by statistical learning models.
%
Such an attempt to learn patterns from data follows the same
direction as recent research developments in
language translation and drug discovery (cf. 1.1).



\subsection*{1.4 The curse of dimensionality}

Today's neuroimaging methods offer very high resolution in
space (especially fMRI and PET)
and
time (especially EEG and MEG).
(Amunts et al., 2014 Science; Buzsaki \& Draguhn, 2009 Science).
%
The mere number of features poses serious
statistical challenges to the investigator.
It is the neuroscientific version of what Richard Bellman
called the \textit{curse of dimensionality} (1961).
%
At the root of the problem,
all data samples look virtually identical
in high-dimensional data scenarios.
%
Accustomed to regularities in 3D neighborhoods,
human intuition is often led astray in
how data behave in
input spaces with an extreme number of variables.



The more dimensions an input space spans,
the further the data points are away from each other
(Hastie et al., 2011).
Conter-intuitively,
measuring the distance between a randomly selected data point
and its closest uniformly distributed neighbors,
reveals a shell-like occurrence probability of
these neighbors, rather than a centered probability mass.
%
Put differently,
when approximating a hypersphere by a surrounding hypercube,
the probability mass of the hypercube
would almost entirely lie outside the hypersphere
(Domingos, 2012).
%
Put in yet another way,
a space divided into isotropic units grows exponentially in the
unit number with linearly increasing dimensionality.
As the main practical conclusion,
the amount of data necessary to populate these units
also grows exponentially
(Bishop, PRML).



Additionally,
the target function is almost always unknown
in statistical learning investigations.
Hence, we have no knowledge of whether or not
special structure may exist in the input data that can be exploited.
%
Knowledge of special structure of the phenomenon under study
can reduce both \textit{bias}
(i.e., difference between the target function and
the average of the function space derivable from a model)
and
\textit{variance}
(i.e., difference between the best approximating function 
from the function space and
the average of the function space).
This is a rare opportunity in SL because increasing,
for instance, the model complexity
typically increases the variance and lowers the bias, and vice versa.
%
In particular,
the problem of overfitting in SL has an immediate relationship
with the multiple-comparisons problem in CS
(Domingos, 2012).
%
The \textit{bias-variance decomposition} captures the fundamental
tradeoff in statistical modeling between
approximating the behavior of
the studied phenomenon and
generalizing to newly generated data describing that behavior. 



A peacefully coexisting conceptual framework exists in SL
that is independent
of the unknown target function.
The \textit{Vapnik-Chervonenkis (VC) dimensions}
formalize the circumstances
under which learning processes can be successful (Vapnik, 1989, 1996).
This comprises any instance of learning
from a number of observations
to derive heuristic rules that capture properties of phenomena in nature,
including both learning in humans and machines.
Formally, the VC dimensions measure the
complexity capacity of a class of approximating functions
(i.e., the function space). 
%
Practically, good models have finite VC dimensions
and are therefore capable to generalize to new data.
Bad models have infinite VC dimensions that
are unable to make generalization conclusions on unseen data,
regardless of data quantity.



More concretely,
SL approaches that incorporate locally varying functions
in small \textit{isotropic} neighborhoods
will fail to generalize in high-dimensional data scenarios.
SL approaches that overcome the curse of dimensionality typically
incorporate an explicit or implicit metric for
\textit{anisotropic} neighborhoods
(Hastie et al., 2011).
%
It is the \textit{hyperparameters} that govern the
smoothing behavior of the imposed local neighborhoods.
%
In so doing,
the \textit{hypothesis set} (i.e., each function in the function space
represent a hypothetical solution to
the estimation problem) is hopefully reduced to
a reasonable pre-selection (i.e., \textit{regularization}).
%
Guiding the statistical estimation process by
complexity restrictions can alleviate the curse of dimensionality.
First,
we can deliberately exclude members of the hypothesis set.
Viewed from the bias-variance trade-off, this calibrates
the sweet spot between underfitting and overfitting.
From the perspective of VC dimensions,
the VC dimensions can be reduced and thus the generalization performance
increased.
%
Second, there is an infinity of possiblities to restrict the hypothesis set.
Yet, these choices are typically guided by external knowledge beyond
the data at hand.
%
Third,
different complexity restrictions lead to different
best approximating functions.



In sum,
the choice of any statistical method contraints
the spectrum of possible results and of permissible interpretations.
Any scientific discovery in the brain is only valid in the
context of the complexity restrictions that have been imposed
on the neurobiological phenomenon of interest.
%
No single statistical strategy, be it SL, CS, or other,
can consistently
do better in all neuroscientific investigations
(Wolpert, 1996).
%
The present dissertation
is hence dedicated to the juggling with
complexity restrictions to
neurobiological reality as observed by fMRI scanning.



\subsection*{1.5 Imaging neuroscience}


- Their popularity of SL in neuroimaging increased dramatically in the attempt of "mind-reading" or "decoding" cognitive processes from neural activity patterns (Haynes and Rees, 2005; Kamitani and Tong, 2005)


neuroimaging: nature of the atomic neurobiological substrates
that increase or decreases is subject to debate.



It has been argued that classical functional neuroimaging applications
are insufficient to infer structure-function relationships
without formal modelling (Stephan, 2004).
(e.g. Flechsig 1905; Meynert 1890)
-> towards formalized descriptions of SFRs


\subsection*{1.6 Statistical learning approaches in brain imaging}

HCP / TERA

- Classification approaches were early used in structural neuroimaging (Herndon et al., 1996) and contributed to improved image preprocessing performance (Ashburner and Friston, 2005).



Much of the success of cognitive neuroscience over the past decades has been a result of implementing a mass-univariate analysis of neuroimaging data obtained from human subjects as part of a general linear model (GLM). GLM treats each volumetric pixel, i.e. voxel, as independently to perform serial univariate statistics (Friston et al., 1995). 
Univariate approaches are recognized to be an excellent approximation for topographically localizing activations, i.e. a differential increase of neural activity, in individual voxels. Multivariate (ML) approaches on the other hand can be used to examine responses that are jointly encoded in multiple voxels. In other words, a univariate statistical model considers a single voxel at a time, while a multivariate model considers many voxels simultaneously and can make inferences on distributed responses without requiring strong independence assumptions. 

Statistical methods with non-parametric properties, often summarized as "multivariate pattern analysis" (MVPA), have recently gained popularity in neuroimaging. In particular, ML approaches are well suited for discovering complex structure between high-dimensional neuroimaging data (mostly functional magnetic resonance imaging) and variables of interest (e.g. external stimuli or experimentally imposed cognitive sets). 


For decades histopathology, lesion studies, and invasive research in non-human primates have been the main neuroscientific workhorses. The advent of neuroimaging methods then leveraged unprecedented insight into brain biology. Task-based neuroimaging studies typically attempt to solicit target mental operations in participants by means of sensory, cognitive, or affective paradigms. This research greatly increased our understanding of the healthy and diseased human brain. Yet, such work rests on a set of strict assumptions, including that a) historically inherited psychological categories are useful description systems for neurobiological phenomena, b) target and non-target mental operations can be statistically distinguished by so-called “cognitive subtraction,” and c) a few dozen participants are sufficient to reliably establish fundamental (patho-)physiological mechanisms in the brain. All these assumptions have repeatedly been questioned. Apart from that, structural neuroimaging approaches, such as voxel-based morphometry, may be used to identify locations in which two groups of subjects, such as patients versus controls, differ from each other in local gray matter (GM) volume. While not being affected by the choice of a particular task, these approaches are also frequently criticized for insufficient sample sizes. In addition, they only allow characterizing presence, absence, or a group-mean effect at a particular location. Finally, methods for individual structural analyses are manifold and have repeatedly been in disagreement.

As a complementary methodological family, machine learning (ML) approaches are characterized by a) making the least assumptions possible, b) being more motivated by mathematical models rather than cognitive theory, and c) automatically mining structured knowledge from massive data resources. Given the widely acknowledged intricacies of and slow progress in schizophrenia research, ML methods lend themselves particularly well. They have the potential to decipher and subsequently render explicit biological indices that might assist diagnosis, treatment, and clinical outcome prediction.


Typical big-data problems concern classification or regression of an output variable y with respect to a large number of input parameters x, also called predictor variables or covariates, on the basis of large training sets. 


Note that this curse of dimensionality does not automatically apply to all big-data algorithms. To the contrary, it occasionally turns out helpful to artificially increase the dimensionality of the parameter space in methods like decision trees or support vector machines 

- Their popularity of SL in neuroimaging increased dramatically in the attempt of "mind-reading" or "decoding" cognitive processes from neural activity patterns (Haynes and Rees, 2005; Kamitani and Tong, 2005)


Third, regarding data analysis techniques, it has been proposed to leverage ML as a particularly powerful tool to investigate the neural correlates of two interacting brains (Konvalinka\& Roepstorff 2012). ML promises to extend the representational agenda of fMRI investigations (i.e., activation-based analysis) and to an informational agenda (i.e., pattern-information-based analysis) (Kriegeskorte et al., 2006; Mur et al., 2009). In particular, multivariate ML approaches are able to address the four questions whether, where, when and how (Brodersen, 2009). ML can provide new evidence whether a given type of information is encoded by neural activity, where a given type of information is neurally encoded, when it is processed/generated/bound and how information is neurally processed in the human brain. Moreover, single-individual fMRI analyses already pose a high-dimensional statistical problem. Performing two-individual fMRI analyses thus imposes an even more challenging "curse of dimensionality" (Hastie et al., 2011). Automated statistical learning of robust structure (i.e., informational patterns) in large quantities of (neuroimaging) data is a core feature of ML approaches. In this way, the flexibility and capability of ML algorithms for pattern mining in extensive data resources make it particularly attractive for the statistical mastery of online two-brain interaction.

. This prompted the current drift towards more data-driven (i.e., fewer assumptions), higher-dimensional (i.e., more features per observation) neuroimaging analyses on larger samples of neuroimaging data (i.e., more observations).


Pietsch:
The notion of a qualitative change from hierarchical to horizontal modeling is further corroborated by a concurrent paradigm shift in statistics, which has been described as a transition from parametric to non-parametric modeling (e.g. Russell \& Norvig 2010, Ch. 18.8), from data to algorithmic models (Breiman 2001), or from model-based to model-free approaches.


As argued in the following chapters,
we need to discover
the low-dimensional manifolds that are
embedded within the high-dimensional neuroimaging data.

Helmstaedter 2011 / deep learning


\section*{2 Unsupervised modelling of brain regions}

\subsection*{2.1 Motivation}
\subsection*{2.2 Methodological approach}
\subsection*{2.3 Experimental results}
\subsection*{2.4 Discussion}



\section*{3 Supervised modelling of brain networks}

\subsection*{3.1 Motivation}
\subsection*{3.2 Methodological approach}
\subsection*{3.3 Experimental results}
\subsection*{3.4 Discussion}


\section*{4 Semi-supervised modelling for structure discovery
and structure inference}

\subsection*{4.1 Motivation}
\subsection*{4.2 Methodological approach}
\subsection*{4.3 Experimental results}
\subsection*{4.4 Discussion}



\section*{5 Conclusion}



\bigskip
\section*{6 References}

\bibliographystyle{plainnat}
\bibliography{danilos_endnote_list}

\end{document}
