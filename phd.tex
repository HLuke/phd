\documentclass[authoryear,review,3p]{elsarticle}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[normalem]{ulem}

\usepackage{lineno,hyperref}
\usepackage{amsmath,amsfonts,amssymb}

\usepackage{listings}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage[acronym,nomain]{glossaries}

\begin{document}
  
\begin{frontmatter}

\title{Statistical Learning of Biological Structure in the Human Brain}

\author{Dr. med. Danilo Bzdok\\
Assistant professor of social and affective neuroscience}

\end{frontmatter}

%\center{
"'But above all, master technique and produce original data; 
all the rest will follow."'

Santiago Ram\'{o}n y Cajal
%}

\bigskip

\newpage
\section*{Publications related to the present dissertation}
\linebreak
\textit{Original papers}

\textbf{Bzdok D}, Grisel O, Eickenberg M, Thirion B, Varoquaux G.
Semi-supervised Factored Logistic Regression of High-Dimensional
Neuroimaging Data. Under review at NIPS.

\textbf{Bzdok D}, Grisel O, Eickenberg M, Varoquaux G, Poupon C, Thirion B.
Network-network architecture: Generative models of task activity patterns.
Under review at Cerebral Cortex.

Bludau S*, \textbf{Bzdok D*}, Gruber O,
Kohn N, Riedl V, Müller V, Hoffstaedter F, Eickhoff SB.
Medial prefrontal aberrations in major depressive disorder
revealed by cytoarchitonically informed voxel-based morphometry.
\textit{American Journal of Psychiatry}, in press. *equal contributions

\textbf{Bzdok D*}, Hartwigsen G*, Reid A, Eickhoff SB.
A hierarchy of the left inferior parietal lobe in social cognition and
language.
\textit{Neuroscience and Biobehavioral Reviews}, in press. *equal contributions

\textbf{Bzdok D}, Heeger A, Langner R, Laird A, Fox P, Palomero-Gallagher,
Vogt BA, Zilles K, Eickhoff SB.
Subspecialization in the human posterior medial cortex.
\textit{Neuroimage}, in press.

Eickhoff SB, Laird AR, Fox PT, \textbf{Bzdok D}*, Hensel L*.
Functional segregation of the human dorsomedial prefrontal cortex.
\textit{Cerebral Cortex}, in press. *equal contributions

\textbf{Bzdok D}, Langner R, Schilbach L, Laird AR, Fox PT, Zilles K, Eickhoff SB.
Characterization of the temporo-parietal junction by combining data-driven
parcellation, complementary connectivity analyses, and functional decoding.
\textit{Neuroimage}, 2013.

\bigskip
\textit{Review and opinion papers}

Eickhoff SB, Thirion B, Varoquaux G, \textbf{Bzdok D}.
Connectivity-based parcellation: critique \& implications.
\textit{Human Brain Mapping}, in press.

Eickhoff, SB \& \textbf{Bzdok D}.
Neuroimaging and modeling. Where is the road to clinical application?
\texit{Der Psychiater}, 2014, in press. 

Eickhoff SB \& \textbf{Bzdok D}.
[Statistical meta-analyses in imaging neuroscience.]
\textit{Klinische Neurophysiologie}, 2013, 44:199-203.

\bigskip
\textit{Book chapters}

\linebreak
\textbf{Bzdok D} \& Eickhoff SB.
Statistical learning of the neurobiological of schizophrenia.
In: \textit{The neurobiology of schizophrenia}, Springer, Heidelberg.

\newpage

\section*{1 Introduction}

\subsection*{1.1 Analytical versus heuristic views of nature}

The world around us is complex and volatile.
%
A large proportion of human research efforts are meditated by the 
"the unreasonable effectiveness of mathematics in the natural sciences."
This was phrased by the Hungarian-American
physicist, mathematician, and Nobel laureate Eugene P. Wigner (1960).
The language of mathematics is a powerful tool to
describe, formalize, and predict phenomena in nature.
The author emphasizes that it is not imperative that
natural regularities exist in the world. He goes on to
say that it might be even more surprising that humans can actually
find and use these regularities to their advantage.
%
Starting from simple axiomes we have derived always more complicated
properties of and relationships between mathematical objects by formal proofs
(Connes A., "A view of mathematics").
A logical pyramid of theoreoms is built that lead to always
more general asserations.
We also have detailed knowledge of the limitations of these mathematical
assertions.
%
On the one hand,
an identical regularity can often be equally well described in very distant
branches of mathematics.
On the other hand,
identical mathematical conclusions have also reemerged from derivation of
a priori unrelated assertions.
%
Indeed, the same formal language has proofed very apt in
the study of completely unrelated topics and diverging scientific disciplines.
This includes movements of celestial objects in the universe studied in astronomy,
metabolism pathways governing the inner life of the cell studied in biochemistry,
and
ABC
.
%
Many rules about the world can thus be perfectly grasped.
As a simple example,
Fibonacci numbers (1, 1, 2, 3, 5, 8, 13, etc.)
reappear in many natural phenomena.
The number of petals of a flower and the spirals of a pinapple tend
to be a Fibonacci sequence.
The family tree of honey bees is also governed by Fibonacci regularities.
Even the proportions of human finger bones follow this formalism.
Knowledge of this mathematical regularity
allows to impose logical structure on the external world.
%
It remains an unresolved question whether we have
\textit{discovered} or \textit{invented} mathematics.
Yet, there is probably no doubt
that mathematical conceptualization
evolves as a feature of human cultural evolution (Tomasello, 2001).
Even the most abstract mathematical concepts can
be exchanged between individuals. Consequently,
they can be easily exchanged between generations and geographical distances.
It is interesting to note that there is usually consensus among mathematicians
about the architecture of their discipline.
In sum, mathematical formalism
appears to be one of the most
powerful and defining tools of the human species
(S. Dehaene, "The Number Sense").
Eugene Wigner concludes the above cited paper with the following words:
"The miracle of the appropriateness of the language of mathematics
for the formulation of the laws of physics is a wonderful gift
which we neither understand nor deserve. We should be grateful for
it and hope that it will remain valid in future research [...]."
(1960, p. 14).


However, this dogma has repeatedly been challenged in the most recent years
(Halevy et al., 2009).

omnipotence of mathematics



classes of phenomena

Every probabilistic model is a superset of a deterministic model (because the deterministic model could be seen as a probabilistic model where the probabilities are restricted to be 0 or 1)

possible paradigm shift

Peter Norvig 
American computer scientist

modern heuristic models can have millions of parameters

Many phenomena in science are stochastic, and the simplest model of them is a probabilistic model; I believe therefore that probabilistic models are our best tool for representing facts about language, for algorithmically processing language, and for understanding how humans process langu


Norvig:
-> insight: simple models fed by large quantities of data perform
better than sophisticated models fed by scarce quantities.
-> Whether a mathematical formalism describes nature well or badly
depends on the quality and quantity of the available data resources.
-> Observing a phenomenon in nature a sufficient number of times
might be sufficient to extract the regularities of its behavior.

Data-driven results can suffer from difficulty to interpret them.

learned staitsical translation models on non-annotated raw
input.
The quality of the data is outweighed by their quantity.
condition probabilities of n-grams.
informal accounts of English grammar count several thousand pages.
statistical machine translation

Machine translation:  of major systems are trained and probabilistic, mostly relying on
probabilistic hidden Markov models.

All current search engines work this way.

The class of hidden markov models (HMM) become a dominanting feature.
- a heuristic, probabilistic mathematical model.

advertisement and politics

rule and human-imposed logic/grammar systems never achieved
convincing success (P. Norvig, 2011, "On Chomsky").
Such heuristic models might indeed better represent the reality of
human communication.

modelling the world

Computational linguistic has turned into a 


Similar switch may be expected in algorithmic trading, self-driving
cars.

humans create machines

But I made the switch: after about 14 years of trying to get language models to work using logical rules, I started to adopt probabilistic approaches


give three examples

- language
- taxics

Protein-Protein Interaktion
SOLL SYMBOLISIESEN
aus Aminosäure-Sequenz allein-> Problem: aus dieser 1D Struktur die 3D Faltung des Proteins abzuleiten/vorherzusagen. 30.000 Protein Strukturen k anderen nicht
momentan v.a. Protein Interaktions-orte identifzieren
Die Ermittlung der natürlichen Proteinstruktur mit physikalischen Methoden ist zwar für viele, aber bei weitem nicht alle, Proteine möglich und zeitlichem Aufwand verbunden.
Pharma-Industrie:
ohne biologisches Wissen
Vorhersagen ist auf mehreren Abstraktionsebenen möglich:
Von „Primärstruktur“ -> Quartärstruktur vorhersagen; Sekundär: Wasserstoff-brücken zwischen benachbarte Teilen von AS; Tertiär: 3D Struktur, genaue Atompositionen darin
Quartär: Protein-Einheit -> Protein-Protein Interaktion vorhersgaen
Wichtige Biologische Strukturen, die ML finden soll: Bindungsstellen, Faltungsart,
„Chemoinformatics"
-> eines der wichtigsten Ziele der Bioinformatik und theoretischen Chemie, diese Eigenschaften von Protein zu entdecken, ohne die Struktur zu kennen - mit AS-Sequenz allein; schneller -> rausfinden, ob ein Protein toxisch oder nicht sein wird: kürzlich deep learning / neuronales Netzwerk -> bessere Vorhersage als alles was in Academia oder Industrie jemals vorher gab im Team hatte Ausbildung in Biologie oder Lebenswissenschaften oder Pharmazie; 2 Wochen investiert


-> dieses andere Beispiele haben uns gezeigt, dass
schlechtes Modell mit vielen Daten ist besser als gutes Modell mit wenig Daten
reletiviert nun eben gerade das Primat der Mathematik. Derjenige macht die besten Übestzne, der die größte Menge und beste Qualitäten an der die besten Annahmen



Their program does not decode the numbers. ReCaptcha works by showing the user two images, only one of which is known to the system. The user the known control and which is the genuinely unknown, so has to solve both. If the control image is solved correctly, it is assumed that the submitted correct too, so the system updates its database with this value
etwas allge
digitalization von alten Büchern (bei denen sich herkömmliche Algos schwertun mögen)
Google has been using reCAPTCHA to digitize content for Google Books


von algorithmischer Modellierung: das Phänomen in der Natur, das wir hier als wenig bekannt komplex und teilweise vielleicht nicht



"Big-data is not interesting, if it's not for ML."
"ML is a set of software algorithms that learn patterns in data without explicit programming model."
"We have to relax our presumptions of control. Because we can guide the process, but we have no precise control over it." "You do not control what you make but you control the process of its creation."
"We can process things that exceed beyond human comprehension - As a more humble approach to the world." "Tradionally prosaic, non-tec businesses will be revolutionized by applying big-data and applying ML."
The Punkt ist - es gibt aktuell vielleicht keine Alternative.

Pietsch:
big-data science requires (i)data representing all relevant configurations of the examined phenomenon. For complex phenomena, this implies high-dimensional data, i.e. data sets in large number of observations covering a wide range of combinations of these parameters.
For many phenomena there exists a relatively sudden change when data-driven approaches become effective (Halevy et al. 2009)—a transition point that could be called adata thresh explanation for its existence: ‘For many tasks, once we have a billion or so examples, we essentially have a closed set that represents (or at least approximates) what we need, withou
On the other hand, the models used in big-data science are often much simpler than the elaborate theoretical structures developed by humans mainly for the purpose of data reduction represent a system, there is no need for complex, hierarchical models anymore—at least with respect topredictions. As Halevy et al write: ‘invariably, simple models and a lot of data on less data.’ (2009, 9) Before providing an overview of the characteristics that distinguish big-data modeling from more conventional scientific modeling let us first take a look at t differences.


For many phenomena there exists a relatively sudden change when data-driven approaches become effective (Halevy et al. 2009)—a transition point that could be called a data threshold. 

Halevy et al give a plausible explanation for its existence: ‘For many tasks, once we have a billion or so examples, we essentially have a closed set that represents (or at least approximates) what we need, without generative rules.’


Pietsch:
The notion of a qualitative change from hierarchical to horizontal modeling is further corroborated by a concurrent paradigm shift in statistics, which has been described as a transition from parametric to non-parametric modeling (e.g. Russell & Norvig 2010, Ch. 18.8), from data to algorithmic models (Breiman 2001), or from model-based to model-free approaches. 

While the parametric model consists in a relatively simple mathematical equation, the non-parametric model consists in all the data and an algorithmic procedure for making predictions.

The difference between the two types of models is striking: While parametric models usually are simple equations, the non-parametric models consist in the original data plus an algorithm to derive predictions from the data.




\subsection*{1.2 Two cultures of statistical modelling}

Statistik - die vielleicht erfolgreichste aller Informationstechnologien




BESCHREIB EINE BEGINNENDE SPALTUNG IN DER STATISTISCHEN MODELLIERUNG VON NATURPHÄNOMENEN
Second the algorithmic modeling culture (subscribed to by 2% of statisticians and many researchers in biology, artificial intelligence, and other fields that deal with complex pheno black box cannot necessarily be described by a simple model. Complex algorithmic approaches (such as support vector machines or boosted decision trees or deep belief networks) a maps from input to output variables, but we have no expectation that the form of the function that emerges from this complex algorithm reflects the true underlying nature.
First the data modeling culture (to which, Breiman estimates, 98% of statisticians subscribe) holds that nature can be described as a black box that has a relatively simple underlyin variables to output variables (with perhaps some random noise thrown in). It is the job of the statistician to wisely choose an underlying model that reflects the reality of nature, and t parameters of the model.


Breiman (2001)


unbox the black box

external world <-> brain




3 components that need to apply when deciding whether a certain problem is a possible target for ML a) a pattern exists (If there is no pattern, then there is no harm in trying ML.)

b) we can not pin down the pattern analytically/mathematically (if we can pin down the problem mathematically GENAU FASSEN K use ML, but it might not create the best model)
c) we have data on the problem



1. es existiert ein Muster

2. wir können das Muster nicht analytisch greifen

3. wir haben Daten zu diesem Problem



Univariate(top-down): (benutzt von Sta6s6kern an Universitäten)
Classical statistics was fashioned for small problems, a few hundred data points at most, a few parameters.
- Datamodelling: Vor-Annahme eines stochas5schen Datenmodels in der Blackbox
Deterministic Newtonian science is majestic, and the basis of modern science too, but a few hundred years of it pretty much exhausted nature’s storehouse of precisely predictable medicine, and economics require a more flexible scientific world view, the kind we statisticians are trained to understand.
-98% aller Sta6s6ker
-Ergebnis: wahrscheinlich, mit das angenommene Modell (H0-Modell) korrekt ist, berühmter p-Wert -> Aussage über dieMechnismen Daten/Natur
-Wich5g: wenn das angenommene Voraussetzungen für die Modell-tests, das den Daten zugrunde liegt, nicht s5mmt, sind auch die hinfällig
-Model wird vom Forscher erzeugt
-Opera6onalisierungist vor Modelbildung/testungnö6g, was ja viele Freiheitsgrade hat
-B: Cox-Model de facto in der medizinischen Literatur etabliert für die Berechnung von Überlebensraten; andere Modelle passen aber > poli6sche und gesellschaYlicher Auswirkungen
- „leapoffate“
Breiman: "This enterprise has at its heart the belief thatastatistician,byimaginationandbylooking atthedata,caninventareasonablygoodpara- metric class of models for a complex mechanism devise "Suppose two statisticians, each one with a different approach to data modeling, fit a model to the same data set. Assume also that each one applies standard goodness-of-fit tests, looks at residual fits the data. Yet the two models give different pictures of nature’s mechanism and lead to different conclusions."
„Thismaysignifythatasdatabecomesmore complex,thedatamodelsbecomemorecumbersome andarelosingtheadvantageofpresentingasimple andclearpictureofnature’smechanism." Ursprünglich designed um auf einem Mechanischen Taschenrechner durchführbar zu sein.

job of the statistician to wisely choose an underlying model that reflects the reality of nature, and then use statistical data to estimate the parameters of the model.

the conclusions made by data modeling are about the model, not about nature

The problem is, if the model does not emulate nature well, then the conclusions may be wrong. 

Parametric methods usually presuppose considerable modeling assumptions. In particular, they summarize the data in terms of a ‘small’ number of model parameters specifying for example a Gaussian distribution or linear dependence, hence the name. 




Mul6variate (bo]om-up): (benutzt von Industriesta6s6kern und jungen Informa5kern/Physiker/Ingenieure; mit Bereich „künstl

Befreiung von Glocken-förmigen Verteilungsannahmen
-algorithmicmodelling: algorithmische Datenmodellierung behandelt die DateneigenschaYen als unbekannt -> Inhalt der Blackbox ist mysteriös
-Classifica6on is the analogue of regression when the variable being predicted is discrete, rather than con6nuous
-2% aller Sta5s5ker
-eine Funk5on erschaffen, die aus x auf welchem Wege auch immeryvorhersagen kann
-Ergebnis: konkreter Prozentsatz, wie oY der vorher gelernte Algorithmus eine unbekanntes/ungesehenesdatensetklassifizieren kann; is simply the frac6on of examples in the test set for which the correct label was predicted -Blackboxzwarunbekannt,aberüberErfoplgundMisserfolgunterschiedlicherKategorienvonClassfifiernkanndieMachnismenderBlackboxin -Model wird vom Algorithmus erzeugt
- studiertes Phänomen hoch komplex und in Teilen vielleicht nicht zu verstehen
-keine subjek6veOpera6onilisierungnö6g
-Nachteil:Vohersageerfolgund Einfachheit/Interpre6erbarkeit im Konflikt -> machen zwar keine Annahmen über die Blackbox, sind abe -nicht unbedingt zwangsläufig besser, aber komplexere Perspek6ve auf diesePhänonemeder Natur
- >100 Daten eigener, 100 und mehre Parameter
"Predictionisrarelyperfect.Thereareusu- allymanyunmeasuredvariableswhoseeffectis referred to as “noise.” But the extent to which the model box emulates nature’s box is a measure the natural phenomenon producing the data."
".Ithasevolvednaturallyinenvironments with lots of data and lots of decisions. But you"

Second the algorithmic modeling culture (subscribed to by 2% of statisticians and many researchers in biology, artificial intelligence, and other fields that deal with complex phenomena), which holds that nature's black box cannot necessarily be described by a simple model. Complex algorithmic approaches (such as support vector machines or boosted decision trees or deep belief networks) are used to estimate the function that maps from input to output variables, but we have no expectation that the form of the function that emerges from this complex algorithm reflects the true underlying nature.

it is that they produce a form that, while accurately modeling reality, is not easily interpretable by humans, and makes no claim to correspond to the generative process used by nature. In other words, algorithmic modeling describes what does happen, but it doesn't answer the question of why.

Instead he asks us to be satisfied with a function that accounts for the observed data well, and generalizes to new, previously unseen data well, but may be expressed in a complex mathematical form that may bear no relation to the "true" function's form (if such a true function even exists).

presupposes few modeling assumptions, e.g. allows for a wide range of functional dependencies or of distribution functions.

data representing all relevant configurations of the examined phenomenon.

In non-parametric modeling, predictions are calculated on the basis of all data. There is no detour over a parametric model that summarizes the data in terms of a few parameters. iii) While thisrenders non-parametric modeling quite flexible with the ability to quickly react to unexpected data, it also becomes extremely data- and calculation-intensive. This aspect accounts for the fact that non-parametric modeling is a relatively recent phenomenon in scientific method.

Non-parametric models allow for novel ways to deal with complexity:
Conventional parametric modeling in terms of equations, describing for example functional dependencies or distribution functions, already presupposes that the picture has been reduced to a small number of parameters and to relatively simple functional relationships. By contrast, algorithmic modeling does not have such restrictions. It relies less on sophisticated mathematics and more on a brute-force execution of a large number of steps, when for example an algorithm searches a large data-base for similar cases. 

By contrast, parametric modeling usually emphasizes understanding. While parametric modeling often correlates with a realist and reductionist viewpoint, non-parametric modeling has instrumentalist and pluralist connotations.


Gauß distribution are chosen by mere mathematical convenience

reach conclusions


A statistical model expresses relationships between a set of variables
whose parameters are learned by training data points.



\subsection*{1.3 The human brain as a complex phenomenon}

Opportunities lie for example in medicine

Bild von einem meiner Kollegen aus Paris entwendet
trotz 200 Jahren Hirnforschungs immer noch keine universelle Hirntheorie zu einigen, die auf von Hirnforschern unterschiedlicher gebiete al würde;
-> hat den unangenehmen Effekt, dass besonders in neurowiss. Systembiologie sehr schwer genau Hypothesen zu entwerfen, di eman dann gezielt experimentell testen und so
Sich den Mechanismen von neuronalener Verarbeitung mit einem Minimum an nötigen Vornahmen, AUS DATEN lernen zu nähern, ist daher ein besonderen attraktiver Zugang



Biology has a small legacy of abstract theorizing.
The present thesis is an attempt to the study the brain using
heuristic, data-driven approaches.

a intrinsically probabilistic process
-> perception depends on the phase of intrinsic oscillations cycles

unresolved nature-nurture debate

consciousness


epistemology/brain




\subsection*{1.4 Imaging neuroscience}






\subsection*{1.5 Statistical learning approaches in brain imaging}





\subsection*{1.6 the curse of dimensionality????}




Typical big-data problems concern classification or regression of an output variable y with respect to a large number of input parameters x, also called predictor variables or covariates, on the basis of large training sets. 


Note that this curse of dimensionality does not automatically apply to all big-data algorithms. To the contrary, it occasionally turns out helpful to artificially increase the dimensionality of the parameter space in methods like decision trees or support vector machines 





\section*{2 Unsupervised modelling of brain regions}

\subsection*{2.1 Motivation}
\subsection*{2.2 Methodological approach}
\subsection*{2.3 Experimental results}
\subsection*{2.4 Discussion}



\section*{3 Supervised modelling of brain networks}

\subsection*{3.1 Motivation}
\subsection*{3.2 Methodological approach}
\subsection*{3.3 Experimental results}
\subsection*{3.4 Discussion}


\section*{4 Semi-supervised modelling for structure discovery
and structure inference}

\subsection*{4.1 Motivation}
\subsection*{4.2 Methodological approach}
\subsection*{4.3 Experimental results}
\subsection*{4.4 Discussion}




\section*{5 Conclusion}






\bigskip
\section*{6 References}

\bibliographystyle{plainnat}
\bibliography{danilos_endnote_list}

\end{document}
