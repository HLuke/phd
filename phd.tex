\documentclass[authoryear,review,3p]{elsarticle}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[normalem]{ulem}

\usepackage{lineno,hyperref}
\usepackage{amsmath,amsfonts,amssymb}

\usepackage{listings}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{hyperref}
\usepackage[acronym,nomain]{glossaries}

\begin{document}
  
\begin{frontmatter}

\title{Statistical Learning of Biological Structure in the Human Brain}

\author{Dr. med. Danilo Bzdok}

\end{frontmatter}

\bigskip
\bigskip
\bigskip
\centerline{
"But above all, master technique and produce original data; 
all the rest will follow."}
\centerline{Santiago Ram\'{o}n y Cajal}

\bigskip

\newpage
\section*{Publications related to the present dissertation}
\linebreak
\textit{Original papers}

\textbf{Bzdok D}, Grisel O, Eickenberg M, Thirion B, Varoquaux G.
Semi-supervised Factored Logistic Regression for High-Dimensional
Neuroimaging Data. Under review at NIPS.

\textbf{Bzdok D}, Grisel O, Eickenberg M, Varoquaux G, Poupon C, Thirion B.
Network-network architecture: Generative models of task activity patterns.
Under review at Cerebral Cortex.

Bludau S*, \textbf{Bzdok D*}, Gruber O,
Kohn N, Riedl V, MÃ¼ller V, Hoffstaedter F, Eickhoff SB.
Medial prefrontal aberrations in major depressive disorder
revealed by cytoarchitonically informed voxel-based morphometry.
\textit{American Journal of Psychiatry}, in press. *equal contributions

\textbf{Bzdok D*}, Hartwigsen G*, Reid A, Eickhoff SB.
A hierarchy of the left inferior parietal lobe in social cognition and
language.
\textit{Neuroscience and Biobehavioral Reviews}, in press. *equal contributions

\textbf{Bzdok D}, Heeger A, Langner R, Laird A, Fox P, Palomero-Gallagher,
Vogt BA, Zilles K, Eickhoff SB.
Subspecialization in the human posterior medial cortex.
\textit{Neuroimage}, in press.

Eickhoff SB, Laird AR, Fox PT, \textbf{Bzdok D}*, Hensel L*.
Functional segregation of the human dorsomedial prefrontal cortex.
\textit{Cerebral Cortex}, in press. *equal contributions

\textbf{Bzdok D}, Langner R, Schilbach L, Laird AR, Fox PT, Zilles K, Eickhoff SB.
Characterization of the temporo-parietal junction by combining data-driven
parcellation, complementary connectivity analyses, and functional decoding.
\textit{Neuroimage}, 2013.

\bigskip
\textit{Review and opinion papers}

Eickhoff SB, Thirion B, Varoquaux G, \textbf{Bzdok D}.
Connectivity-based parcellation: critique \& implications.
\textit{Human Brain Mapping}, in press.

Eickhoff, SB \& \textbf{Bzdok D}.
Neuroimaging and modeling. Where is the road to clinical application?
\texit{Der Psychiater}, 2014, in press. 

Eickhoff SB \& \textbf{Bzdok D}.
[Statistical meta-analyses in imaging neuroscience.]
\textit{Klinische Neurophysiologie}, 2013, 44:199-203.

\bigskip
\textit{Book chapters}

\linebreak
\textbf{Bzdok D} \& Eickhoff SB.
Statistical learning of the neurobiological of schizophrenia.
In: \textit{The neurobiology of schizophrenia}, Springer, Heidelberg.

\newpage

\section*{1 Introduction}

\subsection*{1.1 Analytical versus heuristic views of nature}

The world around us is complex and volatile.
%
A large proportion of human research efforts are meditated by the 
"the unreasonable effectiveness of mathematics in the natural sciences."
This was phrased by the Hungarian-American
physicist, mathematician, and Nobel laureate Eugene P. Wigner (1960).
The language of mathematics is a powerful tool to
describe, formalize, and predict phenomena in nature.
The author emphasizes that it is not imperative that
natural regularities exist in the world. He goes on to
say that it might be even more surprising that humans can actually
find these regularities and use them to their advantage.
%
Starting from human-conceived axiomes
we have derived always more complicated
properties of and relationships between mathematical objects by formal proofs
(Connes A., "A view of mathematics").
A logical pyramid of theoreoms is built that lead to always
more general asserations.
We also have detailed knowledge of the limitations of these mathematical
assertions.
%
On the one hand,
an identical regularity can often be equally well described in very distant
branches of mathematics.
On the other hand,
identical mathematical conclusions have also reemerged from derivation of
a priori unrelated assertions.
%
Indeed, the same formal language has proofed very apt in
the study of completely unrelated topics and diverging scientific disciplines.
This includes movements of celestial objects in the universe studied in astronomy,
metabolism pathways governing the inner life of the cell studied in biochemistry,
and
ABC.
%
Many rules about the world can thus be perfectly grasped.
As another simple example,
Fibonacci numbers (1, 1, 2, 3, 5, 8, 13, etc.)
reappear in many natural phenomena.
The number of petals of a flower and the spirals of a pinapple tend
to be Fibonacci sequences.
The family tree of honey bees is also governed by Fibonacci regularities.
Even the proportions of human finger bones follow this formalism.
Knowledge of such mathematical regularities
allows to impose logical structure on the external world.
%
It remains an unresolved philosophical debate whether we have
\textit{discovered} or \textit{invented} mathematics.
Yet, there is probably no doubt
that mathematical conceptualization
evolves as a feature of human cultural evolution (Tomasello, 2001).
Even the most abstract mathematical concepts can
be exchanged between individuals. Consequently,
they can be easily passed on across generations and geographical distances.
It is interesting to note that
there is usually consensus among mathematicians
about the architecture of their discipline.
From an anthropological perspective,
mathematical formalism
appears to be one of the most
powerful tools and most defining properties of the human species
(S. Dehaene, "The Number Sense").
Indeed,
Eugene Wigner concludes his praise of equations with the following words:
"The miracle of the appropriateness of the language of mathematics
for the formulation of the laws of physics is a wonderful gift
which we neither understand nor deserve. We should be grateful for
it and hope that it will remain valid in future research [...]."
(1960, p. 14).


However, this dogma has repeatedly been challenged in the last years
(Halevy et al., 2009; Hinton/LeCun/Bengio, 2015; Pietsch, 201?).
In different scientific domains,
the omnipotence of mathematical equations has been challenged
as the
best known way to describe and predict nature.
%
This shift in the scientific discourse is made explicit by
three recent empirical observations:
\begin{enumerate}
  \item Sophisticated, more accurate models can be outperformed by
  simple models that are fit with massive training data.
  \item Simple models trained with rich data can outperform
  models that have been designed according to
  extensive domain expertise.
  \item There appears to be a minimal threshold of training data
  for deriving models that suddenly exhibit emergence properties.
\end{enumerate}

One prominent example for such heuristic modelling of nature is
automatic translation of human languages.
Translation systems based on 
human-derived systems of grammar rules have perhaps never achieved
satisfactory success (P. Norvig, 2011, "On Chomsky").
That is, analytical approaches based on
a many thousand pages of domain expertise in form of
deterministic rules appeared insufficient for building
language models that can cope with real-world settings.
%
In contrast, statistical machine translation was more successful
by implementing probabilistic Hidden Markov models
as a heuristic approach.
.
The class of hidden markov models (HMM)
thus became a dominanting feature
of computational linguistics.
%
Today, virtually all professional translation software solutions
for both written and spoken language
are enable by heuristic mathematical models.
%
%
%
As an example from human biology,
we currently have only few means to predict the
toxicity of environmental chemicals and
potential effects of new drug compounds on health.
%
The complex and unknown phenonon in nature here pertains
to the causal link from
a protein's \textit{known} primary structure
(i.e., 1D string of aminoacids)
to
its \textit{unknown} tertiary/quaternary structure
(i.e., the 3D folding of the aminoacid string that subserves function).
%
Of the millions of existing proteins
we only know the tertiary/quaternary structure of
\tilde 30.000 ones.
This is however necessary to identify the position of
interation and bindings sites
for protein-protein interaction.
Only knowledge of these sites in 3D space is crucially important
to infer that protein's interplay with the human body.
%
In this case,
the learning problem is to
derive a computational model from
massive pairs of known primary protein structure and
known protein properties, including toxicity,
by intentionally treating the 3D structure
as a black box.
%
State-of-the-art neural network algorithms
have very recently solved this heuristic learning problem
better than any previous approach in acadmia or industry
(Dahl et al., 2014; Unterthiner et al., 2015).
These investigators thus showed that biological activity
can be reliably predicted from single aminoacid strings
even without recourse to biological domain expertise.
%
There might currently not exist
an analytical counterpart to such
structure-function mapping.
%
%
Similar probabilistic scenarios would include predicting
political outcomes,
optimizing advertisement strategies,
algorithmic trading in stock markets, and
controlling self-driving cars.
Humans can create machines to derive
algorithmic predictions from the data.
%
%
In sum,
observing a phenomenon in nature a sufficient number of times
might be sufficient to
algorithmically extract
the heuristics of its behavior.
This has recently entailed a shift
of attentional focus from model complexity to data complexity.
In the absence of ab analytical alternative access,
simple mathematical models can thus automatically
formalize diverse classes of natural phenomena
depending on the quality and quantity of the available data resources.


\subsection*{1.2 Two cultures of statistical modelling}

Statistics is a branch of mathematics that has arguably been the
overall most successful information science.
%
Given its eclectic character, it may come as no surprise that statistics
has developed both analytical and heuristic strategies
to model regularities of phenomena in nature.
Yet, analytical and heuristic statistical cultures
have emerged independently (Breiman, 2011).
They divere with regard to historical origin, mathematical foundation,
and conceptual purpose.

The overwhelming majority of statisticians
follow an analytical regime by
adhering to \textit{classical statistics} (CS) for
\textit{data modelling}.
They hold that the phenomenon under study can be viewed as a black box
whose inner workings can be described by a small set of
underlying variables.
It is up to the statistician in charge
to choose the model that best reflects nature.
Data is then used to estimate the parameters of that pre-specified model.
Classical statistics have dominated research at the universities
for almost 90 years now.
%
Well known members of the CS family include for instance
Student's t-test, ANOVA,
and Chi-squared test.
\textit{Statistical hypothesis testing} has been introduced in the beginning
of the last century (Fisher, 1925; Neyman and Pearson, 1928).
The same approach is still practiced today in its original form (Goodman, 1999).  
%
The ensuing \textit{p-value} measures how likely it is
to observe the data at hand
assuming the non-preferred null hypothesis ($H_0$)
to find indirect evidence
for the preferred alternative hypothesis ($H_1$).
%
Despite the prevailing presence of p-values,
it has not been conceived by Fisher as an acid test
to judge existing versus non-existing effects in nature.
Rather, the intention was a preliminary tool to
filter which potential effects should be more explicitly tested (Nuzzo, 2014).
%
Notably, if the hand-selected model is a bad description of
the natural phenomenon under study,
the drawn conclusions may be wrong.
%
Nevertheless, statistical hypothesis testing probably fit perfectly
in its time of conception and adoption.
It was designed for use with mechanical calculators
(Efron and Tibshirani, 1991). Gaussian distributional assumptions
have been very useful in many instances for
mathematical convenience and, hence, computational tractability.
Additionally, it suited perfectly the Popperian view of
\textbf{critical empiricism} in academic discourse:
scientific progress is to be made by continuous replacement of current
hypotheses by always more pertinent hypotheses
using verification and falsification (193X).
The rationale behind hypothesis falsification
is that even a lot of evidence cannot confirm
a given theory in an inductive way, 
while a single counter example is able to proof a theory wrong in a deductive way.
%
In sum,
classical statistics was mostly fashioned
for problems with few data points that can be modeled 
by plausible models with a small number of parameters chosen by the
investigator.


In contrast, only a small minority of statisticians
follow a heuristic regime by
adhering to \textit{statistical learning} (SL) for
\textit{algorithmic modelling}.
This statistical framework is frequently adopted by computer scientists,
physicists, ingenieurs, and others without formal statistical brackground
that are typically working in industry
rather than academia (cf. Daniel and Wood, 1971).
%
They hold that natural phenomena
can be studied by estimating regularities in the inputs and
outputs to the black box without making assumptions
about its interal "true" mechanisms.
A statistical model is thus derived that expresses
relationships between the input and output variables
whose parameters are learned by training data.
Put differently, a new function with potentially thousands of
parameters is created
that can predict the output from the input alone.
The input data thus need to represent
all relevant configurations of the examined phenomenon in nature.
%
Please note that SL here summarizes the seemingly more specific terms
data-mining, pattern recognition, artificial intelligence,
and machine learning that are often employed inconsistently.
Members of
the SL family include k-means clustering,
Lasso/Ridge regression, and support vector machine classification.
The independent historical origin of CS and SL families is even
witnessed by the most basic terminology. 
In the CS literature inputs to statistical modeling
are classically called independent variables or, more recently, predictors,
while these are commonly referred to as features
in the SL literature (Hastie et al., 2011).
%
When evaluating whether a certain problem is a possible target for SL
three main aspects come into play (Abu-Mostafa, 200X):
a) a pattern exists
(if there is no pattern, then there is no harm in trying SL),
b) we cannot pin down the pattern analytically
(otherwise one can still apply SL, but it might not create the best model),
and
c) we have data on the problem
(the more, the better).
%
This statistical framework led to a surge of new
computer-intensive statistical techniques since 1980
that are less concerned with mathematical tractability (Efron, 1991).
In the 95'ies suppor vector machines (SVMs)
proofed to be better than the back then de-facto-standard neural networks
in both classification and regression problems (Valpnik, 19??).
This development has been flanked by changing properties of datasets that
are always higher-dimensional (i.e., more features per observation)
and
based
on larger samples of neuroimaging data (i.e., more observations).
This is a trend that is not specific to
neuroimaging research but also takes places
in other scientific disciplines,
including weather forecasting and economic predictions
(Manyika et al., 2011).
In sum,
statistical learning was mostly fashioned
for problems with many data points with largely unknown mechansisms
that are emulated by a mathematical function
created on-the-fly by a machine.


It is also important to appreciate that certain statistical
methods cannot be easily categorized by the CS-SL distinction.
Statistical methods span a continuum between the two poles of CS and SL.
Nevertheless, the two families of statistical methods can be distinguished by a number of properties.
Bayesian statistics do for instance are orthogonal to the CS-SL distinction
and can be adopted in both methodological families in various flavors.
Neither can the terms univariate versus multivariate
(i.e., relying on one versus more than one input variable)
be clearly grouped into either CS or SL.
%
Importantly, neither CS nor SL can generally be considered superior.
This is captured by the \textit{no free lunch theorem}
stating that no single statistical strategy can#
consistently do better in all circumstances (Domingos, 2012; Wolpert, 1996).
The challenge relies in choosing
the statistical approach that is best suited to the neurobiological phenomenon under study and the neuroscientific research object at hand.


Regarding their conceptual purpose, CS and SL exhibit various differences.
CS typically aims at modeling the black box by making a set of
accurate assumptions about its content,
e.g. the nature of the signal distribution.
Contrarily, SL typically aims at finding any way to model
the output of the black box from its input
while making the least assumptions possible (Abu-Mostafa et al., 2012).
In CS the phenomenon is therefore treated as partly known
(i.e., the stochastic processes that generated the data),
whereas in SL the phenomenon is treated as complex,
completely unknown, and partly unknowable.
It is in this way that CS tends to be
analytical
(i.e., imposing mathematical rigor on the phenomenon),
whereas SL tends to be
heuristic
(i.e., finding useful approximations to the phenomenon).
CS assumes a given statistical model at the beginning of the investigation,
whereas in SL the model is
generated in the process of the statistical investigation.
In more formal terms,
CS therefore closely relates to parametric statistics
for confirmatory data analysis
whereas SL closely relates to non-parametric statistics
for exploratory data analysis.
In more practical terms, CS is typically applied to experimental data,
where the variables were controlled by the investigator
(i.e., system under studied is perturbed),
while SL is typically applied to observational
data without such structured influence
(i.e., system is left unperturbed) (Domingos, 2012).
The work unit for CS is the quantified
significance associated with a statistical
relationship between few variables given a pre-specified model.
The work unit for SL is the quantified robustness of patterns
between many variables or, more generally,
the robustness of \texit{special structure} in the data (Hastie et al., 2011).
CS therefore tests for a particular structure in the data,
whereas SL explores and discovers structure in the data.
Formally, CS implements data modeling by
imposing an a priori model in a top-down fashion,
whereas SL implements algorithmic modeling by fitting
a model as a function of the data at hand in a bottom-up fashion.
%
Intuitively, the truth is believed
to be in the model (cf. Wigner, 1960) in the CS-contrained world,
while it is believed to be in the data
(cf. Halevy et al., 2009) in a SL-constrained world.
As a drastically oversimplified, yet useful, conclusion,
CS preassumes and tests \textit{a model for the data},
whereas SL learns \textit{a model from the data}.
%
Taken together,
CS assumes that the data behave according to known mechanisms,
whereas SL exploits
computer algorithms to avoid data mechanism specifications.


\subsection*{1.3 The human brain as a complex phenomenon}

- studiertes PhaÌnomen hoch komplex und in Teilen vielleicht nicht zu verstehen

Opportunities lie for example in medicine

biologicla plausabiliby

Bild von einem meiner Kollegen aus Paris entwendet
trotz 200 Jahren Hirnforschungs immer noch keine universelle Hirntheorie zu einigen, die auf von Hirnforschern unterschiedlicher gebiete al wuÌrde;
-> hat den unangenehmen Effekt, dass besonders in neurowiss. Systembiologie sehr schwer genau Hypothesen zu entwerfen, di eman dann gezielt experimentell testen und so
Sich den Mechanismen von neuronalener Verarbeitung mit einem Minimum an noÌtigen Vornahmen, AUS DATEN lernen zu naÌhern, ist daher ein besonderen attraktiver Zugang

".It has evolved naturally in environments with lots of data and lots of decisions."

Our current tools generate an abundance of data, but we are not sure how to turn this data into knowledge.

we donât have a unified field theory of the brain


The intricacies any neuroscientists faces when articulating observation of phenomena in the brain appear closely related to Ludwig Wittgenstein's second main book Philosophical Investigations (1953/2001). The late Wittgenstein argues that confusion introduced by human language itself is the origin of most philosophical problems. The grammatical and lexical constraints of human language would be too tight to allow for unequivocal description of the diverse circumstances humans encounter. According to Wittgenstein the meaning of language is primarily defined by its practical use in concrete situations, rather than decontextualized abstractions necessarily pre-shared by the interlocutors. Meaning would not even be primarily engendered by reference to objects or their properties. Rather, meaning emerges as a social event between interacting interlocutors. The important implication here is that words and phrases might not have an objective meaning equally accessible to and understood by everybody. This is all the more the case for language descriptions of phenomena that do not occur in every-day reality. Discussing subtleties of abstract concepts, which can hardly be practically experienced, are frequent subject to ambiguity, thus leading to unnoticed misunderstanding and unresolvable paradoxes (cf. Bostrom, 2002; GÃ¶del, 1931; Watzlawick et al., 1967). Biological processes in the brain are an instance of such not directly experienceable phenomena underdetermined by human language that entail interpretative conundrum.

Trans-disciplinary communication might prove critical for social neuroscience to become successful. But this might necessitate particular sensibility to word usage, language hygiene and variants in conceptualization.

There are more reasons why the words and concepts social neuroscientists use should receive increased attention. Conducting studies based on hypotheses that preassume the biological validity of traditional psychological categories, when, as a first example, comparing brain responses to "emotional" versus "cognitive" stimuli, constitute a strong a priori assumption about how brain networks are organized. Yet, it remains elusive how and to what extent psychological terms, such as emotion and cognition (Pessoa, 2008; Van Overwalle, 2011), map onto regional brain responses (Laird et al., 2009; Mesulam, 1998; Poldrack, 2006). As a second example, it is discussed whether social interaction is realized in the brain by simulation that occurs in response to perceiving others' external behavior (especially body movement), subserved by the so-called mirror neuron system, or by means of an allegedly more abstract conceptualization of others' internal states (i.e., thoughts), which could be subserved by theory-of-mind capacities. This debate makes the implicit, yet significant assumption that those two concepts have a discrete representation in neurobiology. Regarding the simulationist side (i.e. MNS), it is still a matter of debate whether humans have an analogue to the mirror neuron system known to exist in non-human primates (cf. Keysers and Gazzola, 2010). Regarding the theorist side (i.e. MENT), the neural network consistently recruited during theory-of-mind cognition is also consistently involved in an array of diverse psychological processes, including moral thinking, autobiographical memory retrieval and navigation (Bzdok et al., 2012b; Spreng et al., 2009). It should hence be carefully called into question whether terms such as "mirror neuron system" or "theory of mind" are an adequate choice of words to refer to discrete neurobiological processes in humans. Taken together, social neuroscience has so far heavily relied on terms and concepts that have been borrowed more traditional, non-neurobiological scientific disciplines. It is however largely unknown how well they grasp neurobiological processes underlying social interaction (cf. Bzdok et al., 2014; Engemann et al., 2012).

Innovative potential may, therefore, lie in investigating the so-called social brain by using alternative approaches that are less reliant on pre-assumed terms and concepts, but exploratory and data-driven instead. Machine learning (ML) provides such a methodological framework.


Statistical methods with non-parametric properties, often summarized as "multivariate pattern analysis" (MVPA), have recently gained popularity in neuroimaging. In particular, ML approaches are well suited for discovering complex structure between high-dimensional neuroimaging data (mostly functional magnetic resonance imaging) and variables of interest (e.g. external stimuli or experimentally imposed cognitive sets). 

Much of the success of cognitive neuroscience over the past decades has been a result of implementing a mass-univariate analysis of neuroimaging data obtained from human subjects as part of a general linear model (GLM). GLM treats each volumetric pixel, i.e. voxel, as independently to perform serial univariate statistics (Friston et al., 1995). 
Univariate approaches are recognized to be an excellent approximation for topographically localizing activations, i.e. a differential increase of neural activity, in individual voxels. Multivariate (ML) approaches on the other hand can be used to examine responses that are jointly encoded in multiple voxels. In other words, a univariate statistical model considers a single voxel at a time, while a multivariate model considers many voxels simultaneously and can make inferences on distributed responses without requiring strong independence assumptions. 

As a broader conclusion, the neurobiological dynamics underlying social cognition might be primarily a function of i) the specific sensory channel of incoming information, ii) the idiosyncrasies of environmental cues, and iii) the (experimentally imposed) current cognitive set.



It has been argued that classical functional neuroimaging applications
are insufficient to infer structure-function relationships
without formal modelling (Stephan, 2004).
-> towards formalized descriptions of SFRs

the most pertinent structure as measured by
contemporary functional neuroimaging techniques
is the human brain is unknown

- Argumente von DFG Antrag

While parametric modeling often correlates with a realist and reductionist viewpoint, non-parametric modeling has instrumentalist and pluralist connotations.

Biology has a small legacy of abstract theorizing.
The present thesis is an attempt to the study the brain using
heuristic, data-driven approaches.

neurobiological mechanisms

"Big-data is not interesting, if it's not for ML."
"ML is a set of software algorithms that learn patterns in data without explicit programming model."
"We have to relax our presumptions of control. Because we can guide the process, but we have no precise control over it." "You do not control what you make but you control the process of its creation."
"We can process things that exceed beyond human comprehension - As a more humble approach to the world." "Tradionally prosaic, non-tec businesses will be revolutionized by applying big-data and applying ML."


a intrinsically probabilistic process
-> perception depends on the phase of intrinsic oscillations cycles

unresolved nature-nurture debate

Given the lack of focal structural changes in almost all psychiatric dis- eases

consciousness

Instead he asks us to be satisfied with a function that
accounts for the observed data well, and generalizes to new,
previously unseen data well, but may be expressed in a complex mathematical
form that may bear no relation to the "true" function's form
(if such a true function even exists).


In non-parametric modeling, predictions are calculated on the basis of all data. There is no detour over a parametric model that summarizes the data in terms of a few parameters. iii) While thisrenders non-parametric modeling quite flexible with the ability to quickly react to unexpected data, it also becomes extremely data- and calculation-intensive. This aspect accounts for the fact that non-parametric modeling is a relatively recent phenomenon in scientific method.

By contrast, algorithmic modeling does not have such restrictions. It relies less on sophisticated mathematics and more on a brute-force execution of a large number of steps



epistemology/brain

why ML is attractive
nature produces data in a black box whose insides are complex, mysterious, and,atleast,partlyunknowable.Whatisobserved

analogous to language translation and drug discovery



\subsection*{1.4 the curse of dimensionality????}

Richard Bellman

Inanastronomyandstatisticsworkshopthis year, a speaker remarked that in twenty-five years we have gone from being a small sample-size science toaverylargesample-sizescience.Astronomical data bases now contain data on two billion objects comprisingover100terabytesandtherateofnew information is accelerating.

HCP / TERA


- difficulty of finding pertinent hypotheses and increasing motivation towards multi-site studies and ever increasing sample sizes currently challenge the CS foundation of neuroimaging


Pietsch:
The notion of a qualitative change from hierarchical to horizontal modeling is further corroborated by a concurrent paradigm shift in statistics, which has been described as a transition from parametric to non-parametric modeling (e.g. Russell & Norvig 2010, Ch. 18.8), from data to algorithmic models (Breiman 2001), or from model-based to model-free approaches.


\subsection*{1.5 Imaging neuroscience}


- Their popularity of SL in neuroimaging increased dramatically in the attempt of "mind-reading" or "decoding" cognitive processes from neural activity patterns (Haynes and Rees, 2005; Kamitani and Tong, 2005)



Helmstaedter 2011 / deep learning



\subsection*{1.6 Statistical learning approaches in brain imaging}


- Classification approaches were early used in structural neuroimaging (Herndon et al., 1996) and contributed to improved image preprocessing performance (Ashburner and Friston, 2005).





Typical big-data problems concern classification or regression of an output variable y with respect to a large number of input parameters x, also called predictor variables or covariates, on the basis of large training sets. 


Note that this curse of dimensionality does not automatically apply to all big-data algorithms. To the contrary, it occasionally turns out helpful to artificially increase the dimensionality of the parameter space in methods like decision trees or support vector machines 



Third, regarding data analysis techniques, it has been proposed to leverage ML as a particularly powerful tool to investigate the neural correlates of two interacting brains (Konvalinka & Roepstorff 2012). ML promises to extend the representational agenda of fMRI investigations (i.e., activation-based analysis) and to an informational agenda (i.e., pattern-information-based analysis) (Kriegeskorte et al., 2006; Mur et al., 2009). In particular, multivariate ML approaches are able to address the four questions whether, where, when and how (Brodersen, 2009). ML can provide new evidence whether a given type of information is encoded by neural activity, where a given type of information is neurally encoded, when it is processed/generated/bound and how information is neurally processed in the human brain. Moreover, single-individual fMRI analyses already pose a high-dimensional statistical problem. Performing two-individual fMRI analyses thus imposes an even more challenging "curse of dimensionality" (Hastie et al., 2011). Automated statistical learning of robust structure (i.e., informational patterns) in large quantities of (neuroimaging) data is a core feature of ML approaches. In this way, the flexibility and capability of ML algorithms for pattern mining in extensive data resources make it particularly attractive for the statistical mastery of online two-brain interaction.




\section*{2 Unsupervised modelling of brain regions}

\subsection*{2.1 Motivation}
\subsection*{2.2 Methodological approach}
\subsection*{2.3 Experimental results}
\subsection*{2.4 Discussion}



\section*{3 Supervised modelling of brain networks}

\subsection*{3.1 Motivation}
\subsection*{3.2 Methodological approach}
\subsection*{3.3 Experimental results}
\subsection*{3.4 Discussion}


\section*{4 Semi-supervised modelling for structure discovery
and structure inference}

\subsection*{4.1 Motivation}
\subsection*{4.2 Methodological approach}
\subsection*{4.3 Experimental results}
\subsection*{4.4 Discussion}




\section*{5 Conclusion}






\bigskip
\section*{6 References}

\bibliographystyle{plainnat}
\bibliography{danilos_endnote_list}

\end{document}
